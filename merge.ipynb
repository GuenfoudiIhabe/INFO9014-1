{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Coffee_Shop_sales.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m bakery_sales \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBakery_sales.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m Coffee_sales \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCoffee_Shop_sales.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Coffee_Shop_sales.csv'"
     ]
    }
   ],
   "source": [
    "bakery_sales = pd.read_csv('bakery_sales')\n",
    "Coffee_sales = pd.read_csv('Coffee_Shop_sales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the unit_price column by removing the € sign and converting the column to float\n",
    "bakery_sales[\"unit_price\"] = (\n",
    "    bakery_sales[\"unit_price\"]\n",
    "    .str.replace(\"€\", \"\", regex=False)\n",
    "    .str.replace(\",\", \".\", regex=False)\n",
    "    .str.replace(\" \", \"\", regex=False)\n",
    "    .str.replace('\"', \"\", regex=False)\n",
    "    .str.strip()\n",
    "    .astype(float)\n",
    ")\n",
    "#rename the columns to match the target schema\n",
    "bakery_sales = bakery_sales.rename(\n",
    "    columns={\n",
    "        \"ticket_number\": \"transaction_id\",\n",
    "        \"date\": \"transaction_date\",\n",
    "        \"time\": \"transaction_time\",\n",
    "        \"Quantity\": \"transaction_qty\",\n",
    "        \"article\": \"product_detail\",\n",
    "    }\n",
    ")\n",
    "#add the missing columns\n",
    "bakery_sales[\"store_id\"] = None\n",
    "bakery_sales[\"store_location\"] = None\n",
    "bakery_sales[\"product_id\"] = None\n",
    "bakery_sales[\"product_category\"] = \"Bakery\"\n",
    "bakery_sales[\"product_type\"] = None\n",
    "#reorder the columns\n",
    "bakery_sales = bakery_sales[\n",
    "    [\n",
    "        \"transaction_id\",\n",
    "        \"transaction_date\",\n",
    "        \"transaction_time\",\n",
    "        \"transaction_qty\",\n",
    "        \"store_id\",\n",
    "        \"store_location\",\n",
    "        \"product_id\",\n",
    "        \"unit_price\",\n",
    "        \"product_category\",\n",
    "        \"product_type\",\n",
    "        \"product_detail\",\n",
    "    ]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bakery_sales.to_csv(\"bakery_sales_new.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_categories = Coffee_sales['product_category'].unique()\n",
    "print(unique_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bakery_items = Coffee_sales[Coffee_sales['product_category'] == 'Bakery'][['product_type', 'product_detail']].drop_duplicates()\n",
    "print(bakery_items.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bakery_sales_new = pd.read_csv(\"bakery_sales_new.csv\")\n",
    "bakery_items_new = bakery_sales_new[bakery_sales_new['product_category'] == 'Bakery'][['product_detail']].drop_duplicates()\n",
    "print(bakery_items_new['product_detail'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_article_to_category(article):\n",
    "    normalized = article.strip().upper()\n",
    "    \n",
    "    if \"CAFE\" in normalized:\n",
    "        return \"Coffee\", \"Coffee\", article\n",
    "    if normalized == \"THE\":\n",
    "        return \"Tea\", \"Tea\", article\n",
    "    \n",
    "    category = \"Bakery\"\n",
    "    \n",
    "    breads = ['BAGUETTE', 'PAIN', 'BOULE', 'PAIN DE MIE', 'SEIGLE', 'VIK BREAD', 'PAIN BANETTE', 'PAIN NOIR']\n",
    "    viennoiseries = ['PAIN AU CHOCOLAT', 'CROISSANT', 'KOUIGN AMANN', 'BRIOCHE', 'CHAUSSON', 'VIENNOISERIE']\n",
    "    patisseries = ['TARTE', 'ECLAIR', 'FRAISIER', 'ST HONORE', 'MACARON', 'ENTREMETS', 'PARIS BREST', 'ROYAL', 'TROPEZIENNE',\n",
    "                   'FINANCIER', 'CRUMBLE', 'MILLES FEUILLES', 'RELIGIEUSE', 'MERINGUE']\n",
    "    sandwiches = ['SANDWICH', 'SAND JB', 'FORMULE SANDWICH']\n",
    "    beverages = ['BOISSON', 'EAU']\n",
    "    meals = ['PLAT', 'TRAITEUR', 'PLATEAU', 'FORMULE PLAT PREPARE', 'PLATPREPARE']\n",
    "    confections = ['CONFISERIE', 'SUCETTE', 'CHOCOLAT', 'MERINGUE', 'SABLE', 'BONBON']\n",
    "    \n",
    "    if any(word in normalized for word in breads):\n",
    "        prod_type = \"Bread\"\n",
    "    elif any(word in normalized for word in viennoiseries):\n",
    "        prod_type = \"Viennoiserie\"\n",
    "    elif any(word in normalized for word in patisseries):\n",
    "        prod_type = \"Pâtisserie\"\n",
    "    elif any(word in normalized for word in sandwiches):\n",
    "        prod_type = \"Sandwich\"\n",
    "    elif any(word in normalized for word in meals):\n",
    "        prod_type = \"Meal/Traiteur\"\n",
    "    elif any(word in normalized for word in beverages):\n",
    "        prod_type = \"Beverage\"\n",
    "    elif any(word in normalized for word in confections):\n",
    "        prod_type = \"Confectionery\"\n",
    "    else:\n",
    "        prod_type = \"Other\"\n",
    "        \n",
    "    return category, prod_type, article\n",
    "\n",
    "bakery_sales_new[['product_category', 'product_type', 'product_detail']] = bakery_sales_new['product_detail'].apply(\n",
    "    lambda x: pd.Series(map_article_to_category(x))\n",
    ")\n",
    "\n",
    "bakery_sales_new.to_csv(\"bakery_sales_merged.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Create a mapping of product names to product IDs.\n",
    "raw_product_names = [\n",
    "    'BAGUETTE', 'PAIN AU CHOCOLAT', 'PAIN', 'TRADITIONAL BAGUETTE', 'CROISSANT',\n",
    "    'BANETTE', 'BANETTINE', 'SPECIAL BREAD', 'COUPE', 'SAND JB EMMENTAL',\n",
    "    'KOUIGN AMANN', 'BOULE 200G', 'BOULE 400G', 'GAL FRANGIPANE 6P', 'CAMPAGNE',\n",
    "    'MOISSON', 'CAFE OU EAU', 'BRIOCHE', 'CEREAL BAGUETTE', 'SEIGLE', 'COMPLET',\n",
    "    'DIVERS PATISSERIE', 'GAL FRANGIPANE 4P', 'COOKIE', 'FICELLE',\n",
    "    'PAIN AUX RAISINS', 'GAL POMME 6P', 'GAL POMME 4P', 'FINANCIER X5',\n",
    "    'VIK BREAD', 'DIVERS VIENNOISERIE', 'GACHE', 'SANDWICH COMPLET',\n",
    "    'PAIN BANETTE', 'GRAND FAR BRETON', 'QUIM BREAD', 'SPECIAL BREAD KG',\n",
    "    'GD KOUIGN AMANN', 'BOULE POLKA', 'DEMI BAGUETTE', 'CHAUSSON AUX POMMES',\n",
    "    'BAGUETTE GRAINE', 'DIVERS CONFISERIE', 'SUCETTE', 'DIVERS BOULANGERIE',\n",
    "    'BOISSON 33CL', 'PATES', 'FORMULE SANDWICH', 'DIVERS SANDWICHS',\n",
    "    'CROISSANT AMANDES', 'PAIN CHOCO AMANDES', 'SACHET VIENNOISERIE', 'NANTAIS',\n",
    "    'CHOCOLAT', 'PAIN S/SEL', 'FONDANT CHOCOLAT', 'GAL POIRE CHOCO 6P',\n",
    "    'GAL POIRE CHOCO 4P', 'GALETTE 8 PERS', 'SAND JB', 'SACHET DE CROUTON',\n",
    "    'GRANDE SUCETTE', 'DEMI PAIN', 'TARTELETTE', 'FLAN', 'PARIS BREST', 'SAVARIN',\n",
    "    'FLAN ABRICOT', 'BAGUETTE APERO', 'MILLES FEUILLES', 'CHOU CHANTILLY',\n",
    "    'ECLAIR', 'ROYAL 4P', 'TARTE FRUITS 6P', 'TARTE FRUITS 4P', 'NOIX JAPONAISE',\n",
    "    'THE', 'BRIOCHETTE', 'ROYAL 6P', 'ECLAIR FRAISE PISTACHE', '.',\n",
    "    'GD FAR BRETON', 'TRIANGLES', 'TROPEZIENNE', 'TROPEZIENNE FRAMBOISE', 'ROYAL',\n",
    "    'TARTE FRAISE 6P', 'TARTELETTE FRAISE', 'TARTE FRAISE 4PER', 'FRAISIER',\n",
    "    'NID DE POULE', 'TARTELETTE CHOC', 'PAIN DE MIE', 'CRUMBLE', 'FINANCIER',\n",
    "    'DIVERS BOISSONS', 'CAKE', 'VIENNOISE', 'TRAITEUR', 'PAIN GRAINES',\n",
    "    'PLATPREPARE6,50', 'PLATPREPARE5,50', 'PLATPREPARE7,00',\n",
    "    'FORMULE PLAT PREPARE', 'ST HONORE', 'BROWNIES', 'RELIGIEUSE',\n",
    "    'PLATPREPARE6,00', 'DELICETROPICAL', 'CRUMBLECARAMEL OU PISTAE',\n",
    "    'PT NANTAIS', 'GD NANTAIS', 'DOUCEUR D HIVER', 'TROIS CHOCOLAT',\n",
    "    'ARTICLE 295', 'TARTE FINE', 'ENTREMETS', 'BRIOCHE DE NOEL', 'FRAMBOISIER',\n",
    "    'BUCHE 4PERS', 'BUCHE 6PERS', 'GD PLATEAU SALE', 'BUCHE 8PERS',\n",
    "    'PT PLATEAU SALE', 'REDUCTION SUCREES 12', 'PAIN NOIR',\n",
    "    'REDUCTION SUCREES 24', 'BOTTEREAU', 'MERINGUE', 'PALMIER', 'PAILLE',\n",
    "    'PLAT 6.50E', 'PLAT 7.60E', 'PLAT 7.00', 'PLAT', 'PLAT 8.30E', 'FORMULE PATE',\n",
    "    'GUERANDAIS', 'PALET BRETON', 'CARAMEL NOIX', 'MACARON', '12 MACARON',\n",
    "    'ARMORICAIN', 'PLAQUE TARTE 25P', 'SABLE F  P', 'PAIN SUISSE PEPITO',\n",
    "    'TULIPE', 'TARTELETTE COCKTAIL', 'SACHET DE VIENNOISERIE'\n",
    "]\n",
    "\n",
    "# Clean names: upper case and remove spaces.\n",
    "clean_product_names = [name.strip().upper() for name in raw_product_names]\n",
    "\n",
    "# Create a dictionary where each product name is mapped to a unique id.\n",
    "product_map = {name: idx for idx, name in enumerate(clean_product_names, start=101)}\n",
    "\n",
    "# Show the mapping information.\n",
    "print(\"Product Mapping:\")\n",
    "for product, pid in product_map.items():\n",
    "    print(f\"{product}: {pid}\")\n",
    "\n",
    "# Read the cleaned sales file.\n",
    "df = pd.read_csv('bakery_sales_merged.csv')\n",
    "\n",
    "# Check missing values before fixing data.\n",
    "print(\"\\nMissing values before cleaning:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Replace missing store info with default values.\n",
    "df['store_id'] = df['store_id'].fillna(1)\n",
    "df['store_location'] = df['store_location'].fillna('Bakery - Default Location')\n",
    "\n",
    "# Clean product details and format the string.\n",
    "df['product_detail'] = df['product_detail'].astype(str).str.strip().str.upper()\n",
    "\n",
    "# Ensure product_id exists and fill missing ones with 0.\n",
    "if 'product_id' not in df.columns:\n",
    "    df['product_id'] = 0\n",
    "else:\n",
    "    df['product_id'] = df['product_id'].fillna(0)\n",
    "\n",
    "# Map the product details to product IDs using the dictionary.\n",
    "df['product_id'] = df.apply(\n",
    "    lambda row: product_map.get(row['product_detail'], 999) if row['product_id'] == 0 else row['product_id'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Convert transaction ids to integer type.\n",
    "df['transaction_id'] = df['transaction_id'].astype(int)\n",
    "\n",
    "# Show missing values after cleaning.\n",
    "print(\"\\nMissing values after cleaning:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_stores = Coffee_sales[['store_id', 'store_location']].drop_duplicates()\n",
    "unique_tickets = bakery_sales['transaction_id'].unique()\n",
    "\n",
    "# Attribute a random store to each transaction\n",
    "np.random.seed(42)\n",
    "ticket_store_mapping = {ticket: np.random.choice(unique_stores['store_id']) for ticket in unique_tickets}\n",
    "\n",
    "df['store_id'] = bakery_sales['transaction_id'].map(ticket_store_mapping)\n",
    "df['store_location'] = df['store_id'].map(unique_stores.set_index('store_id')['store_location'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data to a csv file.\n",
    "output_filename = 'Bakery_sales_cleaned.csv'\n",
    "df.to_csv(output_filename, index=False)\n",
    "print(f\"\\nCleaning complete. Cleaned data saved as '{output_filename}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Bakery_sales_cleaned.csv\")\n",
    "df['sales_id'] = range(1, len(df) + 1)\n",
    "df.to_csv(\"Bakery_sales_cleaned.csv\", index=False)\n",
    "\n",
    "df2 = pd.read_csv(\"Coffee_Shop_Sales.csv\") \n",
    "df2['sales_id'] = range(1, len(df2) + 1)\n",
    "df2.to_csv(\"Coffee_Shop_Sales.csv\", index=False)\n",
    "\n",
    "print(df['sales_id'].head())\n",
    "print(df2['sales_id'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local Image](erd.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE Store (\n",
    "    store_id INT PRIMARY KEY,\n",
    "    store_location VARCHAR(255)\n",
    ");\n",
    "\n",
    "CREATE TABLE Transactions (\n",
    "    sale_id INT PRIMARY KEY,\n",
    "    transaction_id INT,\n",
    "    transaction_date DATE,\n",
    "    transaction_time TIME,\n",
    "    transaction_qty INT,\n",
    "    store_id INT,\n",
    "    product_id INT,\n",
    "    FOREIGN KEY (store_id) REFERENCES Store(store_id),\n",
    "    FOREIGN KEY (product_id) REFERENCES Product(product_id)\n",
    ");\n",
    "\n",
    "CREATE TABLE Product (\n",
    "    product_id INT PRIMARY KEY,\n",
    "    product_category VARCHAR(255) NOT NULL,  \n",
    "    product_type VARCHAR(255),              \n",
    "    product_detail VARCHAR(255),             \n",
    "    unit_price DECIMAL(10,2) NOT NULL\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected as: ('ontodb',)\n",
      "PostgreSQL version: ('PostgreSQL 16.6 (Ubuntu 16.6-0ubuntu0.24.04.1) on x86_64-pc-linux-gnu, compiled by gcc (Ubuntu 13.2.0-23ubuntu4) 13.2.0, 64-bit',)\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"OntoDb\",\n",
    "    user=\"ontodb\",\n",
    "    password=\"admin\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT current_user;\")\n",
    "print(\"Connected as:\", cursor.fetchone())\n",
    "\n",
    "cursor.execute(\"SELECT version();\")\n",
    "print(\"PostgreSQL version:\", cursor.fetchone())\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS Product (\n",
    "    product_id INT PRIMARY KEY,\n",
    "    product_category VARCHAR(255) NOT NULL,  \n",
    "    product_type VARCHAR(255),              \n",
    "    product_detail VARCHAR(255),             \n",
    "    unit_price DECIMAL(10,2) NOT NULL\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS Store (\n",
    "    store_id INT PRIMARY KEY,\n",
    "    store_location VARCHAR(255)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS Transactions (\n",
    "    sale_id INT PRIMARY KEY,\n",
    "    transaction_id INT,\n",
    "    transaction_date DATE,\n",
    "    transaction_time TIME,\n",
    "    transaction_qty INT,\n",
    "    store_id INT,\n",
    "    product_id INT,\n",
    "    FOREIGN KEY (store_id) REFERENCES Store(store_id),\n",
    "    FOREIGN KEY (product_id) REFERENCES Product(product_id)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m Coffee_sales \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCoffee_Shop_Sales.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Write the bakery sales data into the database.\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mto_sql(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbakery_sales_data\u001b[39m\u001b[38;5;124m'\u001b[39m, con\u001b[38;5;241m=\u001b[39mdb, if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Also add the Coffee_Shop_Sales.csv data into the database.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m Coffee_sales\u001b[38;5;241m.\u001b[39mto_sql(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoffee_sales_data\u001b[39m\u001b[38;5;124m'\u001b[39m, con\u001b[38;5;241m=\u001b[39mdb, if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#Followed online tutorial reference : https://www.geeksforgeeks.org/how-to-insert-a-pandas-dataframe-to-an-existing-postgresql-table/\n",
    "\n",
    "\n",
    "Coffee_sales = pd.read_csv('Coffee_Shop_Sales.csv')\n",
    "\n",
    "# Write the bakery sales data into the database.\n",
    "df.to_sql('bakery_sales_data', con=db, if_exists='replace', index=False)\n",
    "\n",
    "# Also add the Coffee_Shop_Sales.csv data into the database.\n",
    "Coffee_sales.to_sql('coffee_sales_data', con=db, if_exists='replace', index=False)\n",
    "\n",
    "# Query and display 5 rows from the bakery sales table.\n",
    "import psycopg2\n",
    "\n",
    "conn_db = psycopg2.connect(host=\"localhost\", database=\"OntoDb\", user=\"ontodb\", password=\"admin\")\n",
    "conn_db.autocommit = True\n",
    "cursor = conn_db.cursor()\n",
    "\n",
    "print(\"Bakery Sales Data (first 5 rows):\")\n",
    "sql_query = \"SELECT * FROM bakery_sales_data LIMIT 5;\"\n",
    "cursor.execute(sql_query)\n",
    "for row in cursor.fetchall():\n",
    "    print(row)\n",
    "\n",
    "print(\"\\nCoffee Shop Sales Data (first 5 rows):\")\n",
    "sql_query_coffee = \"SELECT * FROM coffee_sales_data LIMIT 5;\"\n",
    "cursor.execute(sql_query_coffee)\n",
    "for row in cursor.fetchall():\n",
    "    print(row)\n",
    "\n",
    "cursor.close()\n",
    "conn_db.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import text\n",
    "\n",
    "with db.connect() as conn:\n",
    "    count_query = text(\"SELECT COUNT(*) FROM bakery_sales_data;\")\n",
    "    total_rows = conn.execute(count_query).scalar()\n",
    "    print(f\"Total rows in bakery sales: {total_rows}\")\n",
    "\n",
    "with db.connect() as conn:\n",
    "    count_query = text(\"SELECT COUNT(*) FROM coffee_sales_data;\")\n",
    "    total_rows = conn.execute(count_query).scalar()\n",
    "    print(\"Total rows in Coffee sales:\", total_rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
