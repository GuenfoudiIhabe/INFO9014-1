{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bakery_sales = pd.read_csv('Bakery_sales.csv')\n",
    "Coffee_sales = pd.read_csv('Coffee_Shop_sales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the unit_price column by removing the € sign and converting the column to float\n",
    "bakery_sales[\"unit_price\"] = (\n",
    "    bakery_sales[\"unit_price\"]\n",
    "    .str.replace(\"€\", \"\", regex=False)\n",
    "    .str.replace(\",\", \".\", regex=False)\n",
    "    .str.replace(\" \", \"\", regex=False)\n",
    "    .str.replace('\"', \"\", regex=False)\n",
    "    .str.strip()\n",
    "    .astype(float)\n",
    ")\n",
    "#rename the columns to match the target schema\n",
    "bakery_sales = bakery_sales.rename(\n",
    "    columns={\n",
    "        \"ticket_number\": \"transaction_id\",\n",
    "        \"date\": \"transaction_date\",\n",
    "        \"time\": \"transaction_time\",\n",
    "        \"Quantity\": \"transaction_qty\",\n",
    "        \"article\": \"product_detail\",\n",
    "    }\n",
    ")\n",
    "#add the missing columns\n",
    "bakery_sales[\"store_id\"] = None\n",
    "bakery_sales[\"store_location\"] = None\n",
    "bakery_sales[\"product_id\"] = None\n",
    "bakery_sales[\"product_category\"] = \"Bakery\"\n",
    "bakery_sales[\"product_type\"] = None\n",
    "#reorder the columns\n",
    "bakery_sales = bakery_sales[\n",
    "    [\n",
    "        \"transaction_id\",\n",
    "        \"transaction_date\",\n",
    "        \"transaction_time\",\n",
    "        \"transaction_qty\",\n",
    "        \"store_id\",\n",
    "        \"store_location\",\n",
    "        \"product_id\",\n",
    "        \"unit_price\",\n",
    "        \"product_category\",\n",
    "        \"product_type\",\n",
    "        \"product_detail\",\n",
    "    ]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bakery_sales.to_csv(\"bakery_sales_new.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Coffee' 'Tea' 'Drinking Chocolate' 'Bakery' 'Flavours' 'Loose Tea'\n",
      " 'Coffee beans' 'Packaged Chocolate' 'Branded']\n"
     ]
    }
   ],
   "source": [
    "unique_categories = Coffee_sales['product_category'].unique()\n",
    "print(unique_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_type       3\n",
      "product_detail    11\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "bakery_items = Coffee_sales[Coffee_sales['product_category'] == 'Bakery'][['product_type', 'product_detail']].drop_duplicates()\n",
    "print(bakery_items.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BAGUETTE' 'PAIN AU CHOCOLAT' 'PAIN' 'TRADITIONAL BAGUETTE' 'CROISSANT'\n",
      " 'BANETTE' 'BANETTINE' 'SPECIAL BREAD' 'COUPE' 'SAND JB EMMENTAL'\n",
      " 'KOUIGN AMANN' 'BOULE 200G' 'BOULE 400G' 'GAL FRANGIPANE 6P' 'CAMPAGNE'\n",
      " 'MOISSON' 'CAFE OU EAU' 'BRIOCHE' 'CEREAL BAGUETTE' 'SEIGLE' 'COMPLET'\n",
      " 'DIVERS PATISSERIE' 'GAL FRANGIPANE 4P' 'COOKIE' 'FICELLE'\n",
      " 'PAIN AUX RAISINS' 'GAL POMME 6P' 'GAL POMME 4P' 'FINANCIER X5'\n",
      " 'VIK BREAD' 'DIVERS VIENNOISERIE' 'GACHE' 'SANDWICH COMPLET'\n",
      " 'PAIN BANETTE' 'GRAND FAR BRETON' 'QUIM BREAD' 'SPECIAL BREAD KG'\n",
      " 'GD KOUIGN AMANN' 'BOULE POLKA' 'DEMI BAGUETTE' 'CHAUSSON AUX POMMES'\n",
      " 'BAGUETTE GRAINE' 'DIVERS CONFISERIE' 'SUCETTE' 'DIVERS BOULANGERIE'\n",
      " 'BOISSON 33CL' 'PATES' 'FORMULE SANDWICH' 'DIVERS SANDWICHS'\n",
      " 'CROISSANT AMANDES' 'PAIN CHOCO AMANDES' 'SACHET VIENNOISERIE' 'NANTAIS'\n",
      " 'CHOCOLAT' 'PAIN S/SEL' 'FONDANT CHOCOLAT' 'GAL POIRE CHOCO 6P'\n",
      " 'GAL POIRE CHOCO 4P' 'GALETTE 8 PERS' 'SAND JB' 'SACHET DE CROUTON'\n",
      " 'GRANDE SUCETTE' 'DEMI PAIN' 'TARTELETTE' 'FLAN' 'PARIS BREST' 'SAVARIN'\n",
      " 'FLAN ABRICOT' 'BAGUETTE APERO' 'MILLES FEUILLES' 'CHOU CHANTILLY'\n",
      " 'ECLAIR' 'ROYAL 4P' 'TARTE FRUITS 6P' 'TARTE FRUITS 4P' 'NOIX JAPONAISE'\n",
      " 'THE' 'BRIOCHETTE' 'ROYAL 6P' 'ECLAIR FRAISE PISTACHE' '.'\n",
      " 'GD FAR BRETON' 'TRIANGLES' 'TROPEZIENNE' 'TROPEZIENNE FRAMBOISE' 'ROYAL'\n",
      " 'TARTE FRAISE 6P' 'TARTELETTE FRAISE' 'TARTE FRAISE 4PER' 'FRAISIER'\n",
      " 'NID DE POULE' 'TARTELETTE CHOC' 'PAIN DE MIE' 'CRUMBLE' 'FINANCIER'\n",
      " 'DIVERS BOISSONS' 'CAKE' 'VIENNOISE' 'TRAITEUR' 'PAIN GRAINES'\n",
      " 'PLATPREPARE6,50' 'PLATPREPARE5,50' 'PLATPREPARE7,00'\n",
      " 'FORMULE PLAT PREPARE' 'ST HONORE' 'BROWNIES' 'RELIGIEUSE'\n",
      " 'PLATPREPARE6,00' 'DELICETROPICAL' 'CRUMBLECARAMEL OU PISTAE'\n",
      " 'PT NANTAIS' 'GD NANTAIS' 'DOUCEUR D HIVER' 'TROIS CHOCOLAT'\n",
      " 'ARTICLE 295' 'TARTE FINE' 'ENTREMETS' 'BRIOCHE DE NOEL' 'FRAMBOISIER'\n",
      " 'BUCHE 4PERS' 'BUCHE 6PERS' 'GD PLATEAU SALE' 'BUCHE 8PERS'\n",
      " 'PT PLATEAU SALE' 'REDUCTION SUCREES 12' 'PAIN NOIR'\n",
      " 'REDUCTION SUCREES 24' 'BOTTEREAU' 'MERINGUE' 'PALMIER' 'PAILLE'\n",
      " 'PLAT 6.50E' 'PLAT 7.60E' 'PLAT 7.00' 'PLAT' 'PLAT 8.30E' 'FORMULE PATE'\n",
      " 'GUERANDAIS' 'PALET BRETON' 'CARAMEL NOIX' 'MACARON' '12 MACARON'\n",
      " 'ARMORICAIN' 'PLAQUE TARTE 25P' 'SABLE F  P' 'PAIN SUISSE PEPITO'\n",
      " 'TULIPE' 'TARTELETTE COCKTAIL' 'SACHET DE VIENNOISERIE']\n"
     ]
    }
   ],
   "source": [
    "bakery_sales_new = pd.read_csv(\"bakery_sales_new.csv\")\n",
    "bakery_items_new = bakery_sales_new[bakery_sales_new['product_category'] == 'Bakery'][['product_detail']].drop_duplicates()\n",
    "print(bakery_items_new['product_detail'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_article_to_category(article):\n",
    "    normalized = article.strip().upper()\n",
    "    \n",
    "    if \"CAFE\" in normalized:\n",
    "        return \"Coffee\", \"Coffee\", article\n",
    "    if normalized == \"THE\":\n",
    "        return \"Tea\", \"Tea\", article\n",
    "    \n",
    "    category = \"Bakery\"\n",
    "    \n",
    "    breads = ['BAGUETTE', 'PAIN', 'BOULE', 'PAIN DE MIE', 'SEIGLE', 'VIK BREAD', 'PAIN BANETTE', 'PAIN NOIR']\n",
    "    viennoiseries = ['PAIN AU CHOCOLAT', 'CROISSANT', 'KOUIGN AMANN', 'BRIOCHE', 'CHAUSSON', 'VIENNOISERIE']\n",
    "    patisseries = ['TARTE', 'ECLAIR', 'FRAISIER', 'ST HONORE', 'MACARON', 'ENTREMETS', 'PARIS BREST', 'ROYAL', 'TROPEZIENNE',\n",
    "                   'FINANCIER', 'CRUMBLE', 'MILLES FEUILLES', 'RELIGIEUSE', 'MERINGUE']\n",
    "    sandwiches = ['SANDWICH', 'SAND JB', 'FORMULE SANDWICH']\n",
    "    beverages = ['BOISSON', 'EAU']\n",
    "    meals = ['PLAT', 'TRAITEUR', 'PLATEAU', 'FORMULE PLAT PREPARE', 'PLATPREPARE']\n",
    "    confections = ['CONFISERIE', 'SUCETTE', 'CHOCOLAT', 'MERINGUE', 'SABLE', 'BONBON']\n",
    "    \n",
    "    if any(word in normalized for word in breads):\n",
    "        prod_type = \"Bread\"\n",
    "    elif any(word in normalized for word in viennoiseries):\n",
    "        prod_type = \"Viennoiserie\"\n",
    "    elif any(word in normalized for word in patisseries):\n",
    "        prod_type = \"Pâtisserie\"\n",
    "    elif any(word in normalized for word in sandwiches):\n",
    "        prod_type = \"Sandwich\"\n",
    "    elif any(word in normalized for word in meals):\n",
    "        prod_type = \"Meal/Traiteur\"\n",
    "    elif any(word in normalized for word in beverages):\n",
    "        prod_type = \"Beverage\"\n",
    "    elif any(word in normalized for word in confections):\n",
    "        prod_type = \"Confectionery\"\n",
    "    else:\n",
    "        prod_type = \"Other\"\n",
    "        \n",
    "    return category, prod_type, article\n",
    "\n",
    "bakery_sales_new[['product_category', 'product_type', 'product_detail']] = bakery_sales_new['product_detail'].apply(\n",
    "    lambda x: pd.Series(map_article_to_category(x))\n",
    ")\n",
    "\n",
    "bakery_sales_new.to_csv(\"bakery_sales_merged.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product Mapping:\n",
      "BAGUETTE: 101\n",
      "PAIN AU CHOCOLAT: 102\n",
      "PAIN: 103\n",
      "TRADITIONAL BAGUETTE: 104\n",
      "CROISSANT: 105\n",
      "BANETTE: 106\n",
      "BANETTINE: 107\n",
      "SPECIAL BREAD: 108\n",
      "COUPE: 109\n",
      "SAND JB EMMENTAL: 110\n",
      "KOUIGN AMANN: 111\n",
      "BOULE 200G: 112\n",
      "BOULE 400G: 113\n",
      "GAL FRANGIPANE 6P: 114\n",
      "CAMPAGNE: 115\n",
      "MOISSON: 116\n",
      "CAFE OU EAU: 117\n",
      "BRIOCHE: 118\n",
      "CEREAL BAGUETTE: 119\n",
      "SEIGLE: 120\n",
      "COMPLET: 121\n",
      "DIVERS PATISSERIE: 122\n",
      "GAL FRANGIPANE 4P: 123\n",
      "COOKIE: 124\n",
      "FICELLE: 125\n",
      "PAIN AUX RAISINS: 126\n",
      "GAL POMME 6P: 127\n",
      "GAL POMME 4P: 128\n",
      "FINANCIER X5: 129\n",
      "VIK BREAD: 130\n",
      "DIVERS VIENNOISERIE: 131\n",
      "GACHE: 132\n",
      "SANDWICH COMPLET: 133\n",
      "PAIN BANETTE: 134\n",
      "GRAND FAR BRETON: 135\n",
      "QUIM BREAD: 136\n",
      "SPECIAL BREAD KG: 137\n",
      "GD KOUIGN AMANN: 138\n",
      "BOULE POLKA: 139\n",
      "DEMI BAGUETTE: 140\n",
      "CHAUSSON AUX POMMES: 141\n",
      "BAGUETTE GRAINE: 142\n",
      "DIVERS CONFISERIE: 143\n",
      "SUCETTE: 144\n",
      "DIVERS BOULANGERIE: 145\n",
      "BOISSON 33CL: 146\n",
      "PATES: 147\n",
      "FORMULE SANDWICH: 148\n",
      "DIVERS SANDWICHS: 149\n",
      "CROISSANT AMANDES: 150\n",
      "PAIN CHOCO AMANDES: 151\n",
      "SACHET VIENNOISERIE: 152\n",
      "NANTAIS: 153\n",
      "CHOCOLAT: 154\n",
      "PAIN S/SEL: 155\n",
      "FONDANT CHOCOLAT: 156\n",
      "GAL POIRE CHOCO 6P: 157\n",
      "GAL POIRE CHOCO 4P: 158\n",
      "GALETTE 8 PERS: 159\n",
      "SAND JB: 160\n",
      "SACHET DE CROUTON: 161\n",
      "GRANDE SUCETTE: 162\n",
      "DEMI PAIN: 163\n",
      "TARTELETTE: 164\n",
      "FLAN: 165\n",
      "PARIS BREST: 166\n",
      "SAVARIN: 167\n",
      "FLAN ABRICOT: 168\n",
      "BAGUETTE APERO: 169\n",
      "MILLES FEUILLES: 170\n",
      "CHOU CHANTILLY: 171\n",
      "ECLAIR: 172\n",
      "ROYAL 4P: 173\n",
      "TARTE FRUITS 6P: 174\n",
      "TARTE FRUITS 4P: 175\n",
      "NOIX JAPONAISE: 176\n",
      "THE: 177\n",
      "BRIOCHETTE: 178\n",
      "ROYAL 6P: 179\n",
      "ECLAIR FRAISE PISTACHE: 180\n",
      ".: 181\n",
      "GD FAR BRETON: 182\n",
      "TRIANGLES: 183\n",
      "TROPEZIENNE: 184\n",
      "TROPEZIENNE FRAMBOISE: 185\n",
      "ROYAL: 186\n",
      "TARTE FRAISE 6P: 187\n",
      "TARTELETTE FRAISE: 188\n",
      "TARTE FRAISE 4PER: 189\n",
      "FRAISIER: 190\n",
      "NID DE POULE: 191\n",
      "TARTELETTE CHOC: 192\n",
      "PAIN DE MIE: 193\n",
      "CRUMBLE: 194\n",
      "FINANCIER: 195\n",
      "DIVERS BOISSONS: 196\n",
      "CAKE: 197\n",
      "VIENNOISE: 198\n",
      "TRAITEUR: 199\n",
      "PAIN GRAINES: 200\n",
      "PLATPREPARE6,50: 201\n",
      "PLATPREPARE5,50: 202\n",
      "PLATPREPARE7,00: 203\n",
      "FORMULE PLAT PREPARE: 204\n",
      "ST HONORE: 205\n",
      "BROWNIES: 206\n",
      "RELIGIEUSE: 207\n",
      "PLATPREPARE6,00: 208\n",
      "DELICETROPICAL: 209\n",
      "CRUMBLECARAMEL OU PISTAE: 210\n",
      "PT NANTAIS: 211\n",
      "GD NANTAIS: 212\n",
      "DOUCEUR D HIVER: 213\n",
      "TROIS CHOCOLAT: 214\n",
      "ARTICLE 295: 215\n",
      "TARTE FINE: 216\n",
      "ENTREMETS: 217\n",
      "BRIOCHE DE NOEL: 218\n",
      "FRAMBOISIER: 219\n",
      "BUCHE 4PERS: 220\n",
      "BUCHE 6PERS: 221\n",
      "GD PLATEAU SALE: 222\n",
      "BUCHE 8PERS: 223\n",
      "PT PLATEAU SALE: 224\n",
      "REDUCTION SUCREES 12: 225\n",
      "PAIN NOIR: 226\n",
      "REDUCTION SUCREES 24: 227\n",
      "BOTTEREAU: 228\n",
      "MERINGUE: 229\n",
      "PALMIER: 230\n",
      "PAILLE: 231\n",
      "PLAT 6.50E: 232\n",
      "PLAT 7.60E: 233\n",
      "PLAT 7.00: 234\n",
      "PLAT: 235\n",
      "PLAT 8.30E: 236\n",
      "FORMULE PATE: 237\n",
      "GUERANDAIS: 238\n",
      "PALET BRETON: 239\n",
      "CARAMEL NOIX: 240\n",
      "MACARON: 241\n",
      "12 MACARON: 242\n",
      "ARMORICAIN: 243\n",
      "PLAQUE TARTE 25P: 244\n",
      "SABLE F  P: 245\n",
      "PAIN SUISSE PEPITO: 246\n",
      "TULIPE: 247\n",
      "TARTELETTE COCKTAIL: 248\n",
      "SACHET DE VIENNOISERIE: 249\n",
      "\n",
      "Missing values before cleaning:\n",
      "transaction_id           0\n",
      "transaction_date         0\n",
      "transaction_time         0\n",
      "transaction_qty          0\n",
      "store_id            234005\n",
      "store_location      234005\n",
      "product_id          234005\n",
      "unit_price               0\n",
      "product_category         0\n",
      "product_type             0\n",
      "product_detail           0\n",
      "dtype: int64\n",
      "\n",
      "Missing values after cleaning:\n",
      "transaction_id      0\n",
      "transaction_date    0\n",
      "transaction_time    0\n",
      "transaction_qty     0\n",
      "store_id            0\n",
      "store_location      0\n",
      "product_id          0\n",
      "unit_price          0\n",
      "product_category    0\n",
      "product_type        0\n",
      "product_detail      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Create a mapping of product names to product IDs.\n",
    "raw_product_names = [\n",
    "    'BAGUETTE', 'PAIN AU CHOCOLAT', 'PAIN', 'TRADITIONAL BAGUETTE', 'CROISSANT',\n",
    "    'BANETTE', 'BANETTINE', 'SPECIAL BREAD', 'COUPE', 'SAND JB EMMENTAL',\n",
    "    'KOUIGN AMANN', 'BOULE 200G', 'BOULE 400G', 'GAL FRANGIPANE 6P', 'CAMPAGNE',\n",
    "    'MOISSON', 'CAFE OU EAU', 'BRIOCHE', 'CEREAL BAGUETTE', 'SEIGLE', 'COMPLET',\n",
    "    'DIVERS PATISSERIE', 'GAL FRANGIPANE 4P', 'COOKIE', 'FICELLE',\n",
    "    'PAIN AUX RAISINS', 'GAL POMME 6P', 'GAL POMME 4P', 'FINANCIER X5',\n",
    "    'VIK BREAD', 'DIVERS VIENNOISERIE', 'GACHE', 'SANDWICH COMPLET',\n",
    "    'PAIN BANETTE', 'GRAND FAR BRETON', 'QUIM BREAD', 'SPECIAL BREAD KG',\n",
    "    'GD KOUIGN AMANN', 'BOULE POLKA', 'DEMI BAGUETTE', 'CHAUSSON AUX POMMES',\n",
    "    'BAGUETTE GRAINE', 'DIVERS CONFISERIE', 'SUCETTE', 'DIVERS BOULANGERIE',\n",
    "    'BOISSON 33CL', 'PATES', 'FORMULE SANDWICH', 'DIVERS SANDWICHS',\n",
    "    'CROISSANT AMANDES', 'PAIN CHOCO AMANDES', 'SACHET VIENNOISERIE', 'NANTAIS',\n",
    "    'CHOCOLAT', 'PAIN S/SEL', 'FONDANT CHOCOLAT', 'GAL POIRE CHOCO 6P',\n",
    "    'GAL POIRE CHOCO 4P', 'GALETTE 8 PERS', 'SAND JB', 'SACHET DE CROUTON',\n",
    "    'GRANDE SUCETTE', 'DEMI PAIN', 'TARTELETTE', 'FLAN', 'PARIS BREST', 'SAVARIN',\n",
    "    'FLAN ABRICOT', 'BAGUETTE APERO', 'MILLES FEUILLES', 'CHOU CHANTILLY',\n",
    "    'ECLAIR', 'ROYAL 4P', 'TARTE FRUITS 6P', 'TARTE FRUITS 4P', 'NOIX JAPONAISE',\n",
    "    'THE', 'BRIOCHETTE', 'ROYAL 6P', 'ECLAIR FRAISE PISTACHE', '.',\n",
    "    'GD FAR BRETON', 'TRIANGLES', 'TROPEZIENNE', 'TROPEZIENNE FRAMBOISE', 'ROYAL',\n",
    "    'TARTE FRAISE 6P', 'TARTELETTE FRAISE', 'TARTE FRAISE 4PER', 'FRAISIER',\n",
    "    'NID DE POULE', 'TARTELETTE CHOC', 'PAIN DE MIE', 'CRUMBLE', 'FINANCIER',\n",
    "    'DIVERS BOISSONS', 'CAKE', 'VIENNOISE', 'TRAITEUR', 'PAIN GRAINES',\n",
    "    'PLATPREPARE6,50', 'PLATPREPARE5,50', 'PLATPREPARE7,00',\n",
    "    'FORMULE PLAT PREPARE', 'ST HONORE', 'BROWNIES', 'RELIGIEUSE',\n",
    "    'PLATPREPARE6,00', 'DELICETROPICAL', 'CRUMBLECARAMEL OU PISTAE',\n",
    "    'PT NANTAIS', 'GD NANTAIS', 'DOUCEUR D HIVER', 'TROIS CHOCOLAT',\n",
    "    'ARTICLE 295', 'TARTE FINE', 'ENTREMETS', 'BRIOCHE DE NOEL', 'FRAMBOISIER',\n",
    "    'BUCHE 4PERS', 'BUCHE 6PERS', 'GD PLATEAU SALE', 'BUCHE 8PERS',\n",
    "    'PT PLATEAU SALE', 'REDUCTION SUCREES 12', 'PAIN NOIR',\n",
    "    'REDUCTION SUCREES 24', 'BOTTEREAU', 'MERINGUE', 'PALMIER', 'PAILLE',\n",
    "    'PLAT 6.50E', 'PLAT 7.60E', 'PLAT 7.00', 'PLAT', 'PLAT 8.30E', 'FORMULE PATE',\n",
    "    'GUERANDAIS', 'PALET BRETON', 'CARAMEL NOIX', 'MACARON', '12 MACARON',\n",
    "    'ARMORICAIN', 'PLAQUE TARTE 25P', 'SABLE F  P', 'PAIN SUISSE PEPITO',\n",
    "    'TULIPE', 'TARTELETTE COCKTAIL', 'SACHET DE VIENNOISERIE'\n",
    "]\n",
    "\n",
    "# Clean names: upper case and remove spaces.\n",
    "clean_product_names = [name.strip().upper() for name in raw_product_names]\n",
    "\n",
    "# Create a dictionary where each product name is mapped to a unique id.\n",
    "product_map = {name: idx for idx, name in enumerate(clean_product_names, start=101)}\n",
    "\n",
    "# Show the mapping information.\n",
    "print(\"Product Mapping:\")\n",
    "for product, pid in product_map.items():\n",
    "    print(f\"{product}: {pid}\")\n",
    "\n",
    "# Read the cleaned sales file.\n",
    "df = pd.read_csv('bakery_sales_merged.csv')\n",
    "\n",
    "# Check missing values before fixing data.\n",
    "print(\"\\nMissing values before cleaning:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Replace missing store info with default values.\n",
    "df['store_id'] = df['store_id'].fillna(1)\n",
    "df['store_location'] = df['store_location'].fillna('Bakery - Default Location')\n",
    "\n",
    "# Clean product details and format the string.\n",
    "df['product_detail'] = df['product_detail'].astype(str).str.strip().str.upper()\n",
    "\n",
    "# Ensure product_id exists and fill missing ones with 0.\n",
    "if 'product_id' not in df.columns:\n",
    "    df['product_id'] = 0\n",
    "else:\n",
    "    df['product_id'] = df['product_id'].fillna(0)\n",
    "\n",
    "# Map the product details to product IDs using the dictionary.\n",
    "df['product_id'] = df.apply(\n",
    "    lambda row: product_map.get(row['product_detail'], 999) if row['product_id'] == 0 else row['product_id'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Convert transaction ids to integer type.\n",
    "df['transaction_id'] = df['transaction_id'].astype(int)\n",
    "\n",
    "# Show missing values after cleaning.\n",
    "print(\"\\nMissing values after cleaning:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>transaction_date</th>\n",
       "      <th>transaction_time</th>\n",
       "      <th>transaction_qty</th>\n",
       "      <th>store_id</th>\n",
       "      <th>store_location</th>\n",
       "      <th>product_id</th>\n",
       "      <th>unit_price</th>\n",
       "      <th>product_category</th>\n",
       "      <th>product_type</th>\n",
       "      <th>product_detail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150040</td>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>08:38</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101</td>\n",
       "      <td>0.90</td>\n",
       "      <td>Bakery</td>\n",
       "      <td>Bread</td>\n",
       "      <td>BAGUETTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150040</td>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>08:38</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>102</td>\n",
       "      <td>1.20</td>\n",
       "      <td>Bakery</td>\n",
       "      <td>Bread</td>\n",
       "      <td>PAIN AU CHOCOLAT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150041</td>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>09:14</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>102</td>\n",
       "      <td>1.20</td>\n",
       "      <td>Bakery</td>\n",
       "      <td>Bread</td>\n",
       "      <td>PAIN AU CHOCOLAT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150041</td>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>09:14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>103</td>\n",
       "      <td>1.15</td>\n",
       "      <td>Bakery</td>\n",
       "      <td>Bread</td>\n",
       "      <td>PAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150042</td>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>09:25</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104</td>\n",
       "      <td>1.20</td>\n",
       "      <td>Bakery</td>\n",
       "      <td>Bread</td>\n",
       "      <td>TRADITIONAL BAGUETTE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   transaction_id transaction_date transaction_time  transaction_qty  \\\n",
       "0          150040       2021-01-02            08:38              1.0   \n",
       "1          150040       2021-01-02            08:38              3.0   \n",
       "2          150041       2021-01-02            09:14              2.0   \n",
       "3          150041       2021-01-02            09:14              1.0   \n",
       "4          150042       2021-01-02            09:25              5.0   \n",
       "\n",
       "   store_id store_location  product_id  unit_price product_category  \\\n",
       "0         3            NaN         101        0.90           Bakery   \n",
       "1         3            NaN         102        1.20           Bakery   \n",
       "2         5            NaN         102        1.20           Bakery   \n",
       "3         5            NaN         103        1.15           Bakery   \n",
       "4         3            NaN         104        1.20           Bakery   \n",
       "\n",
       "  product_type        product_detail  \n",
       "0        Bread              BAGUETTE  \n",
       "1        Bread      PAIN AU CHOCOLAT  \n",
       "2        Bread      PAIN AU CHOCOLAT  \n",
       "3        Bread                  PAIN  \n",
       "4        Bread  TRADITIONAL BAGUETTE  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_stores = Coffee_sales[['store_id', 'store_location']].drop_duplicates()\n",
    "unique_tickets = bakery_sales['transaction_id'].unique()\n",
    "\n",
    "# Attribute a random store to each transaction\n",
    "np.random.seed(42)\n",
    "ticket_store_mapping = {ticket: np.random.choice(unique_stores['store_id']) for ticket in unique_tickets}\n",
    "\n",
    "df['store_id'] = bakery_sales['transaction_id'].map(ticket_store_mapping)\n",
    "df['store_location'] = df['store_id'].map(unique_stores.set_index('store_id')['store_location'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning complete. Cleaned data saved as 'Bakery_sales_cleaned.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned data to a csv file.\n",
    "output_filename = 'Bakery_sales_cleaned.csv'\n",
    "df.to_csv(output_filename, index=False)\n",
    "print(f\"\\nCleaning complete. Cleaned data saved as '{output_filename}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    2\n",
      "2    3\n",
      "3    4\n",
      "4    5\n",
      "Name: sales_id, dtype: int64\n",
      "0    1\n",
      "1    2\n",
      "2    3\n",
      "3    4\n",
      "4    5\n",
      "Name: sales_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Bakery_sales_cleaned.csv\")\n",
    "df['sales_id'] = range(1, len(df) + 1)\n",
    "df.to_csv(\"Bakery_sales_cleaned.csv\", index=False)\n",
    "\n",
    "df2 = pd.read_csv(\"Coffee_Shop_Sales.csv\") \n",
    "df2['sales_id'] = range(1, len(df2) + 1)\n",
    "df2.to_csv(\"Coffee_Shop_Sales.csv\", index=False)\n",
    "\n",
    "print(df['sales_id'].head())\n",
    "print(df2['sales_id'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local Image](erd.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2191229633.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[26], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    CREATE TABLE Store (\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "CREATE TABLE Store (\n",
    "    store_id INT PRIMARY KEY,\n",
    "    store_location VARCHAR(255)\n",
    ");\n",
    "\n",
    "CREATE TABLE Transactions (\n",
    "    sale_id INT PRIMARY KEY,\n",
    "    transaction_id INT,\n",
    "    transaction_date DATE,\n",
    "    transaction_time TIME,\n",
    "    transaction_qty INT,\n",
    "    store_id INT,\n",
    "    product_id INT,\n",
    "    FOREIGN KEY (store_id) REFERENCES Store(store_id),\n",
    "    FOREIGN KEY (product_id) REFERENCES Product(product_id)\n",
    ");\n",
    "\n",
    "CREATE TABLE Product (\n",
    "    product_id INT PRIMARY KEY,\n",
    "    product_category VARCHAR(255) NOT NULL,  \n",
    "    product_type VARCHAR(255),              \n",
    "    product_detail VARCHAR(255),             \n",
    "    unit_price DECIMAL(10,2) NOT NULL\n",
    ");\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
