{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bakery_sales = pd.read_csv('Bakery_sales.csv')\n",
    "Coffee_sales = pd.read_csv('Coffee_Shop_Sales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the unit_price column by removing the € sign and converting the column to float\n",
    "bakery_sales[\"unit_price\"] = (\n",
    "    bakery_sales[\"unit_price\"]\n",
    "    .str.replace(\"€\", \"\", regex=False)\n",
    "    .str.replace(\",\", \".\", regex=False)\n",
    "    .str.replace(\" \", \"\", regex=False)\n",
    "    .str.replace('\"', \"\", regex=False)\n",
    "    .str.strip()\n",
    "    .astype(float)\n",
    ")\n",
    "#rename the columns to match the target schema\n",
    "bakery_sales = bakery_sales.rename(\n",
    "    columns={\n",
    "        \"ticket_number\": \"transaction_id\",\n",
    "        \"date\": \"transaction_date\",\n",
    "        \"time\": \"transaction_time\",\n",
    "        \"Quantity\": \"transaction_qty\",\n",
    "        \"article\": \"product_detail\",\n",
    "    }\n",
    ")\n",
    "#add the missing columns\n",
    "bakery_sales[\"store_id\"] = None\n",
    "bakery_sales[\"store_location\"] = None\n",
    "bakery_sales[\"product_id\"] = None\n",
    "bakery_sales[\"product_category\"] = \"Bakery\"\n",
    "bakery_sales[\"product_type\"] = None\n",
    "#reorder the columns\n",
    "bakery_sales = bakery_sales[\n",
    "    [\n",
    "        \"transaction_id\",\n",
    "        \"transaction_date\",\n",
    "        \"transaction_time\",\n",
    "        \"transaction_qty\",\n",
    "        \"store_id\",\n",
    "        \"store_location\",\n",
    "        \"product_id\",\n",
    "        \"unit_price\",\n",
    "        \"product_category\",\n",
    "        \"product_type\",\n",
    "        \"product_detail\",\n",
    "    ]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bakery_sales.to_csv(\"bakery_sales_new.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Coffee' 'Tea' 'Drinking Chocolate' 'Bakery' 'Flavours' 'Loose Tea'\n",
      " 'Coffee beans' 'Packaged Chocolate' 'Branded']\n"
     ]
    }
   ],
   "source": [
    "unique_categories = Coffee_sales['product_category'].unique()\n",
    "print(unique_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_type       3\n",
      "product_detail    11\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "bakery_items = Coffee_sales[Coffee_sales['product_category'] == 'Bakery'][['product_type', 'product_detail']].drop_duplicates()\n",
    "print(bakery_items.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BAGUETTE' 'PAIN AU CHOCOLAT' 'PAIN' 'TRADITIONAL BAGUETTE' 'CROISSANT'\n",
      " 'BANETTE' 'BANETTINE' 'SPECIAL BREAD' 'COUPE' 'SAND JB EMMENTAL'\n",
      " 'KOUIGN AMANN' 'BOULE 200G' 'BOULE 400G' 'GAL FRANGIPANE 6P' 'CAMPAGNE'\n",
      " 'MOISSON' 'CAFE OU EAU' 'BRIOCHE' 'CEREAL BAGUETTE' 'SEIGLE' 'COMPLET'\n",
      " 'DIVERS PATISSERIE' 'GAL FRANGIPANE 4P' 'COOKIE' 'FICELLE'\n",
      " 'PAIN AUX RAISINS' 'GAL POMME 6P' 'GAL POMME 4P' 'FINANCIER X5'\n",
      " 'VIK BREAD' 'DIVERS VIENNOISERIE' 'GACHE' 'SANDWICH COMPLET'\n",
      " 'PAIN BANETTE' 'GRAND FAR BRETON' 'QUIM BREAD' 'SPECIAL BREAD KG'\n",
      " 'GD KOUIGN AMANN' 'BOULE POLKA' 'DEMI BAGUETTE' 'CHAUSSON AUX POMMES'\n",
      " 'BAGUETTE GRAINE' 'DIVERS CONFISERIE' 'SUCETTE' 'DIVERS BOULANGERIE'\n",
      " 'BOISSON 33CL' 'PATES' 'FORMULE SANDWICH' 'DIVERS SANDWICHS'\n",
      " 'CROISSANT AMANDES' 'PAIN CHOCO AMANDES' 'SACHET VIENNOISERIE' 'NANTAIS'\n",
      " 'CHOCOLAT' 'PAIN S/SEL' 'FONDANT CHOCOLAT' 'GAL POIRE CHOCO 6P'\n",
      " 'GAL POIRE CHOCO 4P' 'GALETTE 8 PERS' 'SAND JB' 'SACHET DE CROUTON'\n",
      " 'GRANDE SUCETTE' 'DEMI PAIN' 'TARTELETTE' 'FLAN' 'PARIS BREST' 'SAVARIN'\n",
      " 'FLAN ABRICOT' 'BAGUETTE APERO' 'MILLES FEUILLES' 'CHOU CHANTILLY'\n",
      " 'ECLAIR' 'ROYAL 4P' 'TARTE FRUITS 6P' 'TARTE FRUITS 4P' 'NOIX JAPONAISE'\n",
      " 'THE' 'BRIOCHETTE' 'ROYAL 6P' 'ECLAIR FRAISE PISTACHE' '.'\n",
      " 'GD FAR BRETON' 'TRIANGLES' 'TROPEZIENNE' 'TROPEZIENNE FRAMBOISE' 'ROYAL'\n",
      " 'TARTE FRAISE 6P' 'TARTELETTE FRAISE' 'TARTE FRAISE 4PER' 'FRAISIER'\n",
      " 'NID DE POULE' 'TARTELETTE CHOC' 'PAIN DE MIE' 'CRUMBLE' 'FINANCIER'\n",
      " 'DIVERS BOISSONS' 'CAKE' 'VIENNOISE' 'TRAITEUR' 'PAIN GRAINES'\n",
      " 'PLATPREPARE6,50' 'PLATPREPARE5,50' 'PLATPREPARE7,00'\n",
      " 'FORMULE PLAT PREPARE' 'ST HONORE' 'BROWNIES' 'RELIGIEUSE'\n",
      " 'PLATPREPARE6,00' 'DELICETROPICAL' 'CRUMBLECARAMEL OU PISTAE'\n",
      " 'PT NANTAIS' 'GD NANTAIS' 'DOUCEUR D HIVER' 'TROIS CHOCOLAT'\n",
      " 'ARTICLE 295' 'TARTE FINE' 'ENTREMETS' 'BRIOCHE DE NOEL' 'FRAMBOISIER'\n",
      " 'BUCHE 4PERS' 'BUCHE 6PERS' 'GD PLATEAU SALE' 'BUCHE 8PERS'\n",
      " 'PT PLATEAU SALE' 'REDUCTION SUCREES 12' 'PAIN NOIR'\n",
      " 'REDUCTION SUCREES 24' 'BOTTEREAU' 'MERINGUE' 'PALMIER' 'PAILLE'\n",
      " 'PLAT 6.50E' 'PLAT 7.60E' 'PLAT 7.00' 'PLAT' 'PLAT 8.30E' 'FORMULE PATE'\n",
      " 'GUERANDAIS' 'PALET BRETON' 'CARAMEL NOIX' 'MACARON' '12 MACARON'\n",
      " 'ARMORICAIN' 'PLAQUE TARTE 25P' 'SABLE F  P' 'PAIN SUISSE PEPITO'\n",
      " 'TULIPE' 'TARTELETTE COCKTAIL' 'SACHET DE VIENNOISERIE']\n"
     ]
    }
   ],
   "source": [
    "bakery_sales_new = pd.read_csv(\"bakery_sales_new.csv\")\n",
    "bakery_items_new = bakery_sales_new[bakery_sales_new['product_category'] == 'Bakery'][['product_detail']].drop_duplicates()\n",
    "print(bakery_items_new['product_detail'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_article_to_category(article):\n",
    "    normalized = article.strip().upper()\n",
    "    \n",
    "    if \"CAFE\" in normalized:\n",
    "        return \"Coffee\", \"Coffee\", article\n",
    "    if normalized == \"THE\":\n",
    "        return \"Tea\", \"Tea\", article\n",
    "    \n",
    "    category = \"Bakery\"\n",
    "    \n",
    "    breads = ['BAGUETTE', 'PAIN', 'BOULE', 'PAIN DE MIE', 'SEIGLE', 'VIK BREAD', 'PAIN BANETTE', 'PAIN NOIR']\n",
    "    viennoiseries = ['PAIN AU CHOCOLAT', 'CROISSANT', 'KOUIGN AMANN', 'BRIOCHE', 'CHAUSSON', 'VIENNOISERIE']\n",
    "    patisseries = ['TARTE', 'ECLAIR', 'FRAISIER', 'ST HONORE', 'MACARON', 'ENTREMETS', 'PARIS BREST', 'ROYAL', 'TROPEZIENNE',\n",
    "                   'FINANCIER', 'CRUMBLE', 'MILLES FEUILLES', 'RELIGIEUSE', 'MERINGUE']\n",
    "    sandwiches = ['SANDWICH', 'SAND JB', 'FORMULE SANDWICH']\n",
    "    beverages = ['BOISSON', 'EAU']\n",
    "    meals = ['PLAT', 'TRAITEUR', 'PLATEAU', 'FORMULE PLAT PREPARE', 'PLATPREPARE']\n",
    "    confections = ['CONFISERIE', 'SUCETTE', 'CHOCOLAT', 'MERINGUE', 'SABLE', 'BONBON']\n",
    "    \n",
    "    if any(word in normalized for word in breads):\n",
    "        prod_type = \"Bread\"\n",
    "    elif any(word in normalized for word in viennoiseries):\n",
    "        prod_type = \"Viennoiserie\"\n",
    "    elif any(word in normalized for word in patisseries):\n",
    "        prod_type = \"Pâtisserie\"\n",
    "    elif any(word in normalized for word in sandwiches):\n",
    "        prod_type = \"Sandwich\"\n",
    "    elif any(word in normalized for word in meals):\n",
    "        prod_type = \"Meal/Traiteur\"\n",
    "    elif any(word in normalized for word in beverages):\n",
    "        prod_type = \"Beverage\"\n",
    "    elif any(word in normalized for word in confections):\n",
    "        prod_type = \"Confectionery\"\n",
    "    else:\n",
    "        prod_type = \"Other\"\n",
    "        \n",
    "    return category, prod_type, article\n",
    "\n",
    "bakery_sales_new[['product_category', 'product_type', 'product_detail']] = bakery_sales_new['product_detail'].apply(\n",
    "    lambda x: pd.Series(map_article_to_category(x))\n",
    ")\n",
    "\n",
    "bakery_sales_new.to_csv(\"bakery_sales_merged.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product Mapping:\n",
      "BAGUETTE: 101\n",
      "PAIN AU CHOCOLAT: 102\n",
      "PAIN: 103\n",
      "TRADITIONAL BAGUETTE: 104\n",
      "CROISSANT: 105\n",
      "BANETTE: 106\n",
      "BANETTINE: 107\n",
      "SPECIAL BREAD: 108\n",
      "COUPE: 109\n",
      "SAND JB EMMENTAL: 110\n",
      "KOUIGN AMANN: 111\n",
      "BOULE 200G: 112\n",
      "BOULE 400G: 113\n",
      "GAL FRANGIPANE 6P: 114\n",
      "CAMPAGNE: 115\n",
      "MOISSON: 116\n",
      "CAFE OU EAU: 117\n",
      "BRIOCHE: 118\n",
      "CEREAL BAGUETTE: 119\n",
      "SEIGLE: 120\n",
      "COMPLET: 121\n",
      "DIVERS PATISSERIE: 122\n",
      "GAL FRANGIPANE 4P: 123\n",
      "COOKIE: 124\n",
      "FICELLE: 125\n",
      "PAIN AUX RAISINS: 126\n",
      "GAL POMME 6P: 127\n",
      "GAL POMME 4P: 128\n",
      "FINANCIER X5: 129\n",
      "VIK BREAD: 130\n",
      "DIVERS VIENNOISERIE: 131\n",
      "GACHE: 132\n",
      "SANDWICH COMPLET: 133\n",
      "PAIN BANETTE: 134\n",
      "GRAND FAR BRETON: 135\n",
      "QUIM BREAD: 136\n",
      "SPECIAL BREAD KG: 137\n",
      "GD KOUIGN AMANN: 138\n",
      "BOULE POLKA: 139\n",
      "DEMI BAGUETTE: 140\n",
      "CHAUSSON AUX POMMES: 141\n",
      "BAGUETTE GRAINE: 142\n",
      "DIVERS CONFISERIE: 143\n",
      "SUCETTE: 144\n",
      "DIVERS BOULANGERIE: 145\n",
      "BOISSON 33CL: 146\n",
      "PATES: 147\n",
      "FORMULE SANDWICH: 148\n",
      "DIVERS SANDWICHS: 149\n",
      "CROISSANT AMANDES: 150\n",
      "PAIN CHOCO AMANDES: 151\n",
      "SACHET VIENNOISERIE: 152\n",
      "NANTAIS: 153\n",
      "CHOCOLAT: 154\n",
      "PAIN S/SEL: 155\n",
      "FONDANT CHOCOLAT: 156\n",
      "GAL POIRE CHOCO 6P: 157\n",
      "GAL POIRE CHOCO 4P: 158\n",
      "GALETTE 8 PERS: 159\n",
      "SAND JB: 160\n",
      "SACHET DE CROUTON: 161\n",
      "GRANDE SUCETTE: 162\n",
      "DEMI PAIN: 163\n",
      "TARTELETTE: 164\n",
      "FLAN: 165\n",
      "PARIS BREST: 166\n",
      "SAVARIN: 167\n",
      "FLAN ABRICOT: 168\n",
      "BAGUETTE APERO: 169\n",
      "MILLES FEUILLES: 170\n",
      "CHOU CHANTILLY: 171\n",
      "ECLAIR: 172\n",
      "ROYAL 4P: 173\n",
      "TARTE FRUITS 6P: 174\n",
      "TARTE FRUITS 4P: 175\n",
      "NOIX JAPONAISE: 176\n",
      "THE: 177\n",
      "BRIOCHETTE: 178\n",
      "ROYAL 6P: 179\n",
      "ECLAIR FRAISE PISTACHE: 180\n",
      ".: 181\n",
      "GD FAR BRETON: 182\n",
      "TRIANGLES: 183\n",
      "TROPEZIENNE: 184\n",
      "TROPEZIENNE FRAMBOISE: 185\n",
      "ROYAL: 186\n",
      "TARTE FRAISE 6P: 187\n",
      "TARTELETTE FRAISE: 188\n",
      "TARTE FRAISE 4PER: 189\n",
      "FRAISIER: 190\n",
      "NID DE POULE: 191\n",
      "TARTELETTE CHOC: 192\n",
      "PAIN DE MIE: 193\n",
      "CRUMBLE: 194\n",
      "FINANCIER: 195\n",
      "DIVERS BOISSONS: 196\n",
      "CAKE: 197\n",
      "VIENNOISE: 198\n",
      "TRAITEUR: 199\n",
      "PAIN GRAINES: 200\n",
      "PLATPREPARE6,50: 201\n",
      "PLATPREPARE5,50: 202\n",
      "PLATPREPARE7,00: 203\n",
      "FORMULE PLAT PREPARE: 204\n",
      "ST HONORE: 205\n",
      "BROWNIES: 206\n",
      "RELIGIEUSE: 207\n",
      "PLATPREPARE6,00: 208\n",
      "DELICETROPICAL: 209\n",
      "CRUMBLECARAMEL OU PISTAE: 210\n",
      "PT NANTAIS: 211\n",
      "GD NANTAIS: 212\n",
      "DOUCEUR D HIVER: 213\n",
      "TROIS CHOCOLAT: 214\n",
      "ARTICLE 295: 215\n",
      "TARTE FINE: 216\n",
      "ENTREMETS: 217\n",
      "BRIOCHE DE NOEL: 218\n",
      "FRAMBOISIER: 219\n",
      "BUCHE 4PERS: 220\n",
      "BUCHE 6PERS: 221\n",
      "GD PLATEAU SALE: 222\n",
      "BUCHE 8PERS: 223\n",
      "PT PLATEAU SALE: 224\n",
      "REDUCTION SUCREES 12: 225\n",
      "PAIN NOIR: 226\n",
      "REDUCTION SUCREES 24: 227\n",
      "BOTTEREAU: 228\n",
      "MERINGUE: 229\n",
      "PALMIER: 230\n",
      "PAILLE: 231\n",
      "PLAT 6.50E: 232\n",
      "PLAT 7.60E: 233\n",
      "PLAT 7.00: 234\n",
      "PLAT: 235\n",
      "PLAT 8.30E: 236\n",
      "FORMULE PATE: 237\n",
      "GUERANDAIS: 238\n",
      "PALET BRETON: 239\n",
      "CARAMEL NOIX: 240\n",
      "MACARON: 241\n",
      "12 MACARON: 242\n",
      "ARMORICAIN: 243\n",
      "PLAQUE TARTE 25P: 244\n",
      "SABLE F  P: 245\n",
      "PAIN SUISSE PEPITO: 246\n",
      "TULIPE: 247\n",
      "TARTELETTE COCKTAIL: 248\n",
      "SACHET DE VIENNOISERIE: 249\n",
      "\n",
      "Missing values before cleaning:\n",
      "transaction_id           0\n",
      "transaction_date         0\n",
      "transaction_time         0\n",
      "transaction_qty          0\n",
      "store_id            234005\n",
      "store_location      234005\n",
      "product_id          234005\n",
      "unit_price               0\n",
      "product_category         0\n",
      "product_type             0\n",
      "product_detail           0\n",
      "dtype: int64\n",
      "\n",
      "Missing values after cleaning:\n",
      "transaction_id           0\n",
      "transaction_date         0\n",
      "transaction_time         0\n",
      "transaction_qty          0\n",
      "store_id            234005\n",
      "store_location      234005\n",
      "product_id               0\n",
      "unit_price               0\n",
      "product_category         0\n",
      "product_type             0\n",
      "product_detail           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Create a mapping of product names to product IDs.\n",
    "raw_product_names = [\n",
    "    'BAGUETTE', 'PAIN AU CHOCOLAT', 'PAIN', 'TRADITIONAL BAGUETTE', 'CROISSANT',\n",
    "    'BANETTE', 'BANETTINE', 'SPECIAL BREAD', 'COUPE', 'SAND JB EMMENTAL',\n",
    "    'KOUIGN AMANN', 'BOULE 200G', 'BOULE 400G', 'GAL FRANGIPANE 6P', 'CAMPAGNE',\n",
    "    'MOISSON', 'CAFE OU EAU', 'BRIOCHE', 'CEREAL BAGUETTE', 'SEIGLE', 'COMPLET',\n",
    "    'DIVERS PATISSERIE', 'GAL FRANGIPANE 4P', 'COOKIE', 'FICELLE',\n",
    "    'PAIN AUX RAISINS', 'GAL POMME 6P', 'GAL POMME 4P', 'FINANCIER X5',\n",
    "    'VIK BREAD', 'DIVERS VIENNOISERIE', 'GACHE', 'SANDWICH COMPLET',\n",
    "    'PAIN BANETTE', 'GRAND FAR BRETON', 'QUIM BREAD', 'SPECIAL BREAD KG',\n",
    "    'GD KOUIGN AMANN', 'BOULE POLKA', 'DEMI BAGUETTE', 'CHAUSSON AUX POMMES',\n",
    "    'BAGUETTE GRAINE', 'DIVERS CONFISERIE', 'SUCETTE', 'DIVERS BOULANGERIE',\n",
    "    'BOISSON 33CL', 'PATES', 'FORMULE SANDWICH', 'DIVERS SANDWICHS',\n",
    "    'CROISSANT AMANDES', 'PAIN CHOCO AMANDES', 'SACHET VIENNOISERIE', 'NANTAIS',\n",
    "    'CHOCOLAT', 'PAIN S/SEL', 'FONDANT CHOCOLAT', 'GAL POIRE CHOCO 6P',\n",
    "    'GAL POIRE CHOCO 4P', 'GALETTE 8 PERS', 'SAND JB', 'SACHET DE CROUTON',\n",
    "    'GRANDE SUCETTE', 'DEMI PAIN', 'TARTELETTE', 'FLAN', 'PARIS BREST', 'SAVARIN',\n",
    "    'FLAN ABRICOT', 'BAGUETTE APERO', 'MILLES FEUILLES', 'CHOU CHANTILLY',\n",
    "    'ECLAIR', 'ROYAL 4P', 'TARTE FRUITS 6P', 'TARTE FRUITS 4P', 'NOIX JAPONAISE',\n",
    "    'THE', 'BRIOCHETTE', 'ROYAL 6P', 'ECLAIR FRAISE PISTACHE', '.',\n",
    "    'GD FAR BRETON', 'TRIANGLES', 'TROPEZIENNE', 'TROPEZIENNE FRAMBOISE', 'ROYAL',\n",
    "    'TARTE FRAISE 6P', 'TARTELETTE FRAISE', 'TARTE FRAISE 4PER', 'FRAISIER',\n",
    "    'NID DE POULE', 'TARTELETTE CHOC', 'PAIN DE MIE', 'CRUMBLE', 'FINANCIER',\n",
    "    'DIVERS BOISSONS', 'CAKE', 'VIENNOISE', 'TRAITEUR', 'PAIN GRAINES',\n",
    "    'PLATPREPARE6,50', 'PLATPREPARE5,50', 'PLATPREPARE7,00',\n",
    "    'FORMULE PLAT PREPARE', 'ST HONORE', 'BROWNIES', 'RELIGIEUSE',\n",
    "    'PLATPREPARE6,00', 'DELICETROPICAL', 'CRUMBLECARAMEL OU PISTAE',\n",
    "    'PT NANTAIS', 'GD NANTAIS', 'DOUCEUR D HIVER', 'TROIS CHOCOLAT',\n",
    "    'ARTICLE 295', 'TARTE FINE', 'ENTREMETS', 'BRIOCHE DE NOEL', 'FRAMBOISIER',\n",
    "    'BUCHE 4PERS', 'BUCHE 6PERS', 'GD PLATEAU SALE', 'BUCHE 8PERS',\n",
    "    'PT PLATEAU SALE', 'REDUCTION SUCREES 12', 'PAIN NOIR',\n",
    "    'REDUCTION SUCREES 24', 'BOTTEREAU', 'MERINGUE', 'PALMIER', 'PAILLE',\n",
    "    'PLAT 6.50E', 'PLAT 7.60E', 'PLAT 7.00', 'PLAT', 'PLAT 8.30E', 'FORMULE PATE',\n",
    "    'GUERANDAIS', 'PALET BRETON', 'CARAMEL NOIX', 'MACARON', '12 MACARON',\n",
    "    'ARMORICAIN', 'PLAQUE TARTE 25P', 'SABLE F  P', 'PAIN SUISSE PEPITO',\n",
    "    'TULIPE', 'TARTELETTE COCKTAIL', 'SACHET DE VIENNOISERIE'\n",
    "]\n",
    "\n",
    "# Clean names: upper case and remove spaces.\n",
    "clean_product_names = [name.strip().upper() for name in raw_product_names]\n",
    "\n",
    "# Create a dictionary where each product name is mapped to a unique id.\n",
    "product_map = {name: idx for idx, name in enumerate(clean_product_names, start=101)}\n",
    "\n",
    "# Show the mapping information.\n",
    "print(\"Product Mapping:\")\n",
    "for product, pid in product_map.items():\n",
    "    print(f\"{product}: {pid}\")\n",
    "\n",
    "# Read the cleaned sales file.\n",
    "df = pd.read_csv('bakery_sales_merged.csv')\n",
    "\n",
    "# Check missing values before fixing data.\n",
    "print(\"\\nMissing values before cleaning:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Clean product details and format the string.\n",
    "df['product_detail'] = df['product_detail'].astype(str).str.strip().str.upper()\n",
    "\n",
    "# Ensure product_id exists and fill missing ones with 0.\n",
    "if 'product_id' not in df.columns:\n",
    "    df['product_id'] = 0\n",
    "else:\n",
    "    df['product_id'] = df['product_id'].fillna(0)\n",
    "\n",
    "# Map the product details to product IDs using the dictionary.\n",
    "df['product_id'] = df.apply(\n",
    "    lambda row: product_map.get(row['product_detail'], 999) if row['product_id'] == 0 else row['product_id'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Convert transaction ids to integer type.\n",
    "df['transaction_id'] = df['transaction_id'].astype(int)\n",
    "\n",
    "# Show missing values after cleaning.\n",
    "print(\"\\nMissing values after cleaning:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>transaction_date</th>\n",
       "      <th>transaction_time</th>\n",
       "      <th>transaction_qty</th>\n",
       "      <th>store_id</th>\n",
       "      <th>store_location</th>\n",
       "      <th>product_id</th>\n",
       "      <th>unit_price</th>\n",
       "      <th>product_category</th>\n",
       "      <th>product_type</th>\n",
       "      <th>product_detail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150040</td>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>08:38</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Astoria</td>\n",
       "      <td>101</td>\n",
       "      <td>0.90</td>\n",
       "      <td>Bakery</td>\n",
       "      <td>Bread</td>\n",
       "      <td>BAGUETTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150040</td>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>08:38</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Astoria</td>\n",
       "      <td>102</td>\n",
       "      <td>1.20</td>\n",
       "      <td>Bakery</td>\n",
       "      <td>Bread</td>\n",
       "      <td>PAIN AU CHOCOLAT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150041</td>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>09:14</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Lower Manhattan</td>\n",
       "      <td>102</td>\n",
       "      <td>1.20</td>\n",
       "      <td>Bakery</td>\n",
       "      <td>Bread</td>\n",
       "      <td>PAIN AU CHOCOLAT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150041</td>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>09:14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Lower Manhattan</td>\n",
       "      <td>103</td>\n",
       "      <td>1.15</td>\n",
       "      <td>Bakery</td>\n",
       "      <td>Bread</td>\n",
       "      <td>PAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150042</td>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>09:25</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Astoria</td>\n",
       "      <td>104</td>\n",
       "      <td>1.20</td>\n",
       "      <td>Bakery</td>\n",
       "      <td>Bread</td>\n",
       "      <td>TRADITIONAL BAGUETTE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   transaction_id transaction_date transaction_time  transaction_qty  \\\n",
       "0          150040       2021-01-02            08:38              1.0   \n",
       "1          150040       2021-01-02            08:38              3.0   \n",
       "2          150041       2021-01-02            09:14              2.0   \n",
       "3          150041       2021-01-02            09:14              1.0   \n",
       "4          150042       2021-01-02            09:25              5.0   \n",
       "\n",
       "   store_id   store_location  product_id  unit_price product_category  \\\n",
       "0         3          Astoria         101        0.90           Bakery   \n",
       "1         3          Astoria         102        1.20           Bakery   \n",
       "2         5  Lower Manhattan         102        1.20           Bakery   \n",
       "3         5  Lower Manhattan         103        1.15           Bakery   \n",
       "4         3          Astoria         104        1.20           Bakery   \n",
       "\n",
       "  product_type        product_detail  \n",
       "0        Bread              BAGUETTE  \n",
       "1        Bread      PAIN AU CHOCOLAT  \n",
       "2        Bread      PAIN AU CHOCOLAT  \n",
       "3        Bread                  PAIN  \n",
       "4        Bread  TRADITIONAL BAGUETTE  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_stores = Coffee_sales[['store_id', 'store_location']].drop_duplicates()\n",
    "unique_tickets = bakery_sales['transaction_id'].unique()\n",
    "\n",
    "# Attribute a random store to each transaction\n",
    "np.random.seed(42)\n",
    "ticket_store_mapping = {ticket: np.random.choice(unique_stores['store_id']) for ticket in unique_tickets}\n",
    "\n",
    "df['store_id'] = bakery_sales['transaction_id'].map(ticket_store_mapping)\n",
    "df['store_location'] = df['store_id'].map(unique_stores.set_index('store_id')['store_location'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data to a csv file.\n",
    "output_filename = 'Bakery_sales_cleaned.csv'\n",
    "df.to_csv(output_filename, index=False)\n",
    "print(f\"\\nCleaning complete. Cleaned data saved as '{output_filename}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    2\n",
      "2    3\n",
      "3    4\n",
      "4    5\n",
      "Name: sales_id, dtype: int64\n",
      "0    1\n",
      "1    2\n",
      "2    3\n",
      "3    4\n",
      "4    5\n",
      "Name: sales_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Bakery_sales_cleaned.csv\")\n",
    "df['sales_id'] = range(1, len(df) + 1)\n",
    "df.to_csv(\"Bakery_sales_cleaned.csv\", index=False)\n",
    "\n",
    "df2 = pd.read_csv(\"Coffee_Shop_Sales.csv\") \n",
    "df2['sales_id'] = range(1, len(df2) + 1)\n",
    "df2.to_csv(\"Coffee_Shop_Sales.csv\", index=False)\n",
    "\n",
    "print(df['sales_id'].head())\n",
    "print(df2['sales_id'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local Image](erd.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE Store (\n",
    "    store_id INT PRIMARY KEY,\n",
    "    store_location VARCHAR(255)\n",
    ");\n",
    "\n",
    "CREATE TABLE Transactions (\n",
    "    sale_id INT PRIMARY KEY,\n",
    "    transaction_id INT,\n",
    "    transaction_date DATE,\n",
    "    transaction_time TIME,\n",
    "    transaction_qty INT,\n",
    "    store_id INT,\n",
    "    product_id INT,\n",
    "    FOREIGN KEY (store_id) REFERENCES Store(store_id),\n",
    "    FOREIGN KEY (product_id) REFERENCES Product(product_id)\n",
    ");\n",
    "\n",
    "CREATE TABLE Product (\n",
    "    product_id INT PRIMARY KEY,\n",
    "    product_category VARCHAR(255) NOT NULL,  \n",
    "    product_type VARCHAR(255),              \n",
    "    product_detail VARCHAR(255),             \n",
    "    unit_price DECIMAL(10,2) NOT NULL\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected as: ('ontodb',)\n",
      "PostgreSQL version: ('PostgreSQL 17.4 (Debian 17.4-1.pgdg120+2) on x86_64-pc-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"OntoDb\",\n",
    "    user=\"ontodb\",\n",
    "    password=\"admin\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT current_user;\")\n",
    "print(\"Connected as:\", cursor.fetchone())\n",
    "\n",
    "cursor.execute(\"SELECT version();\")\n",
    "print(\"PostgreSQL version:\", cursor.fetchone())\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS Product (\n",
    "    product_id INT PRIMARY KEY,\n",
    "    product_category VARCHAR(255) NOT NULL,  \n",
    "    product_type VARCHAR(255),              \n",
    "    product_detail VARCHAR(255),             \n",
    "    unit_price DECIMAL(10,2) NOT NULL\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS Store (\n",
    "    store_id INT PRIMARY KEY,\n",
    "    store_location VARCHAR(255)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS Transactions (\n",
    "    sale_id INT PRIMARY KEY,\n",
    "    transaction_id INT,\n",
    "    transaction_date DATE,\n",
    "    transaction_time TIME,\n",
    "    transaction_qty INT,\n",
    "    store_id INT,\n",
    "    product_id INT,\n",
    "    FOREIGN KEY (store_id) REFERENCES Store(store_id),\n",
    "    FOREIGN KEY (product_id) REFERENCES Product(product_id)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "db = create_engine(\"postgresql://ontodb:admin@localhost/OntoDb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bakery Sales Data (first 5 rows):\n",
      "(150040, '2021-01-02', '08:38', 1.0, 3, 'Astoria', 101, 0.9, 'Bakery', 'Bread', 'BAGUETTE', 1)\n",
      "(150040, '2021-01-02', '08:38', 3.0, 3, 'Astoria', 102, 1.2, 'Bakery', 'Bread', 'PAIN AU CHOCOLAT', 2)\n",
      "(150041, '2021-01-02', '09:14', 2.0, 5, 'Lower Manhattan', 102, 1.2, 'Bakery', 'Bread', 'PAIN AU CHOCOLAT', 3)\n",
      "(150041, '2021-01-02', '09:14', 1.0, 5, 'Lower Manhattan', 103, 1.15, 'Bakery', 'Bread', 'PAIN', 4)\n",
      "(150042, '2021-01-02', '09:25', 5.0, 3, 'Astoria', 104, 1.2, 'Bakery', 'Bread', 'TRADITIONAL BAGUETTE', 5)\n",
      "\n",
      "Coffee Shop Sales Data (first 5 rows):\n",
      "(1, '2023-01-01', '07:06:11', 2, 5, 'Lower Manhattan', 32, 3.0, 'Coffee', 'Gourmet brewed coffee', 'Ethiopia Rg', 1)\n",
      "(2, '2023-01-01', '07:08:56', 2, 5, 'Lower Manhattan', 57, 3.1, 'Tea', 'Brewed Chai tea', 'Spicy Eye Opener Chai Lg', 2)\n",
      "(3, '2023-01-01', '07:14:04', 2, 5, 'Lower Manhattan', 59, 4.5, 'Drinking Chocolate', 'Hot chocolate', 'Dark chocolate Lg', 3)\n",
      "(4, '2023-01-01', '07:20:24', 1, 5, 'Lower Manhattan', 22, 2.0, 'Coffee', 'Drip coffee', 'Our Old Time Diner Blend Sm', 4)\n",
      "(5, '2023-01-01', '07:22:41', 2, 5, 'Lower Manhattan', 57, 3.1, 'Tea', 'Brewed Chai tea', 'Spicy Eye Opener Chai Lg', 5)\n"
     ]
    }
   ],
   "source": [
    "#Followed online tutorial reference : https://www.geeksforgeeks.org/how-to-insert-a-pandas-dataframe-to-an-existing-postgresql-table/\n",
    "\n",
    "\n",
    "Coffee_sales = pd.read_csv('Coffee_Shop_Sales.csv')\n",
    "\n",
    "# Write the bakery sales data into the database.\n",
    "df.to_sql('bakery_sales_data', con=db, if_exists='replace', index=False)\n",
    "\n",
    "# Also add the Coffee_Shop_Sales.csv data into the database.\n",
    "Coffee_sales.to_sql('coffee_sales_data', con=db, if_exists='replace', index=False)\n",
    "\n",
    "# Query and display 5 rows from the bakery sales table.\n",
    "import psycopg2\n",
    "\n",
    "conn_db = psycopg2.connect(host=\"localhost\", database=\"OntoDb\", user=\"ontodb\", password=\"admin\")\n",
    "conn_db.autocommit = True\n",
    "cursor = conn_db.cursor()\n",
    "\n",
    "print(\"Bakery Sales Data (first 5 rows):\")\n",
    "sql_query = \"SELECT * FROM bakery_sales_data LIMIT 5;\"\n",
    "cursor.execute(sql_query)\n",
    "for row in cursor.fetchall():\n",
    "    print(row)\n",
    "\n",
    "print(\"\\nCoffee Shop Sales Data (first 5 rows):\")\n",
    "sql_query_coffee = \"SELECT * FROM coffee_sales_data LIMIT 5;\"\n",
    "cursor.execute(sql_query_coffee)\n",
    "for row in cursor.fetchall():\n",
    "    print(row)\n",
    "\n",
    "cursor.close()\n",
    "conn_db.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in bakery sales: 234005\n",
      "Total rows in Coffee sales: 149116\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import text\n",
    "\n",
    "with db.connect() as conn:\n",
    "    count_query = text(\"SELECT COUNT(*) FROM bakery_sales_data;\")\n",
    "    total_rows = conn.execute(count_query).scalar()\n",
    "    print(f\"Total rows in bakery sales: {total_rows}\")\n",
    "\n",
    "with db.connect() as conn:\n",
    "    count_query = text(\"SELECT COUNT(*) FROM coffee_sales_data;\")\n",
    "    total_rows = conn.execute(count_query).scalar()\n",
    "    print(\"Total rows in Coffee sales:\", total_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: Populate the Schema with  Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "df_a = pd.read_csv(\"Bakery_sales_cleaned.csv\")\n",
    "df_b = pd.read_csv(\"Coffee_Shop_Sales.csv\")\n",
    "df_merged = pd.concat([df_a, df_b], ignore_index=True)\n",
    "\n",
    "tbl_products = df_merged[[\"product_id\",\"product_category\",\"product_type\",\"product_detail\",\"unit_price\"]].drop_duplicates()\n",
    "tbl_stores = df_merged[[\"store_id\",\"store_location\"]].drop_duplicates()\n",
    "tbl_transactions = df_merged[[\"sales_id\",\"transaction_id\",\"transaction_date\",\"transaction_time\",\"transaction_qty\",\"store_id\",\"product_id\"]]\n",
    "\n",
    "cn = psycopg2.connect(host=\"localhost\", database=\"OntoDb\", user=\"ontodb\", password=\"admin\")\n",
    "cr = cn.cursor()\n",
    "\n",
    "sql_products = \"\"\"\n",
    "INSERT INTO Product (product_id, product_category, product_type, product_detail, unit_price)\n",
    "VALUES (%s, %s, %s, %s, %s)\n",
    "ON CONFLICT (product_id) DO NOTHING;\n",
    "\"\"\"\n",
    "sql_stores = \"\"\"\n",
    "INSERT INTO Store (store_id, store_location)\n",
    "VALUES (%s, %s)\n",
    "ON CONFLICT (store_id) DO NOTHING;\n",
    "\"\"\"\n",
    "sql_transactions = \"\"\"\n",
    "INSERT INTO Transactions (sale_id, transaction_id, transaction_date, transaction_time, transaction_qty, store_id, product_id)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "ON CONFLICT (sale_id) DO NOTHING;\n",
    "\"\"\"\n",
    "\n",
    "cr.executemany(sql_products, list(tbl_products.itertuples(index=False, name=None)))\n",
    "cr.executemany(sql_stores, list(tbl_stores.itertuples(index=False, name=None)))\n",
    "cr.executemany(sql_transactions, list(tbl_transactions.itertuples(index=False, name=None)))\n",
    "\n",
    "cn.commit()\n",
    "cr.close()\n",
    "cn.close()\n",
    "\n",
    "print(\"Data loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product Table:\n",
      "(101, 'Bakery', 'Bread', 'BAGUETTE', Decimal('0.90'))\n",
      "(102, 'Bakery', 'Bread', 'PAIN AU CHOCOLAT', Decimal('1.20'))\n",
      "(103, 'Bakery', 'Bread', 'PAIN', Decimal('1.15'))\n",
      "(104, 'Bakery', 'Bread', 'TRADITIONAL BAGUETTE', Decimal('1.20'))\n",
      "(105, 'Bakery', 'Viennoiserie', 'CROISSANT', Decimal('1.10'))\n",
      "(106, 'Bakery', 'Other', 'BANETTE', Decimal('1.05'))\n",
      "(107, 'Bakery', 'Other', 'BANETTINE', Decimal('0.60'))\n",
      "(108, 'Bakery', 'Other', 'SPECIAL BREAD', Decimal('2.40'))\n",
      "(109, 'Bakery', 'Other', 'COUPE', Decimal('0.15'))\n",
      "(110, 'Bakery', 'Sandwich', 'SAND JB EMMENTAL', Decimal('3.50'))\n",
      "(111, 'Bakery', 'Viennoiserie', 'KOUIGN AMANN', Decimal('2.10'))\n",
      "(112, 'Bakery', 'Bread', 'BOULE 200G', Decimal('1.10'))\n",
      "(113, 'Bakery', 'Bread', 'BOULE 400G', Decimal('1.50'))\n",
      "(114, 'Bakery', 'Other', 'GAL FRANGIPANE 6P', Decimal('12.00'))\n",
      "(115, 'Bakery', 'Other', 'CAMPAGNE', Decimal('1.80'))\n",
      "(116, 'Bakery', 'Other', 'MOISSON', Decimal('2.00'))\n",
      "(117, 'Coffee', 'Coffee', 'CAFE OU EAU', Decimal('1.00'))\n",
      "(118, 'Bakery', 'Viennoiserie', 'BRIOCHE', Decimal('5.00'))\n",
      "(119, 'Bakery', 'Bread', 'CEREAL BAGUETTE', Decimal('1.25'))\n",
      "(120, 'Bakery', 'Bread', 'SEIGLE', Decimal('1.80'))\n",
      "(121, 'Bakery', 'Other', 'COMPLET', Decimal('1.50'))\n",
      "(122, 'Bakery', 'Other', 'DIVERS PATISSERIE', Decimal('0.00'))\n",
      "(123, 'Bakery', 'Other', 'GAL FRANGIPANE 4P', Decimal('8.00'))\n",
      "(124, 'Bakery', 'Other', 'COOKIE', Decimal('1.00'))\n",
      "(125, 'Bakery', 'Other', 'FICELLE', Decimal('0.60'))\n",
      "(126, 'Bakery', 'Bread', 'PAIN AUX RAISINS', Decimal('1.40'))\n",
      "(127, 'Bakery', 'Other', 'GAL POMME 6P', Decimal('12.00'))\n",
      "(128, 'Bakery', 'Other', 'GAL POMME 4P', Decimal('8.00'))\n",
      "(129, 'Bakery', 'Pâtisserie', 'FINANCIER X5', Decimal('3.00'))\n",
      "(130, 'Bakery', 'Bread', 'VIK BREAD', Decimal('2.50'))\n",
      "(131, 'Bakery', 'Viennoiserie', 'DIVERS VIENNOISERIE', Decimal('4.00'))\n",
      "(132, 'Bakery', 'Other', 'GACHE', Decimal('5.00'))\n",
      "(133, 'Bakery', 'Sandwich', 'SANDWICH COMPLET', Decimal('4.50'))\n",
      "(134, 'Bakery', 'Bread', 'PAIN BANETTE', Decimal('1.40'))\n",
      "(135, 'Bakery', 'Other', 'GRAND FAR BRETON', Decimal('7.00'))\n",
      "(136, 'Bakery', 'Other', 'QUIM BREAD', Decimal('1.00'))\n",
      "(137, 'Bakery', 'Other', 'SPECIAL BREAD KG', Decimal('4.80'))\n",
      "(138, 'Bakery', 'Viennoiserie', 'GD KOUIGN AMANN', Decimal('7.50'))\n",
      "(139, 'Bakery', 'Bread', 'BOULE POLKA', Decimal('1.60'))\n",
      "(140, 'Bakery', 'Bread', 'DEMI BAGUETTE', Decimal('0.45'))\n",
      "(141, 'Bakery', 'Viennoiserie', 'CHAUSSON AUX POMMES', Decimal('1.40'))\n",
      "(142, 'Bakery', 'Bread', 'BAGUETTE GRAINE', Decimal('1.30'))\n",
      "(143, 'Bakery', 'Confectionery', 'DIVERS CONFISERIE', Decimal('3.00'))\n",
      "(144, 'Bakery', 'Confectionery', 'SUCETTE', Decimal('0.30'))\n",
      "(145, 'Bakery', 'Other', 'DIVERS BOULANGERIE', Decimal('2.00'))\n",
      "(146, 'Bakery', 'Beverage', 'BOISSON 33CL', Decimal('1.50'))\n",
      "(147, 'Bakery', 'Other', 'PATES', Decimal('4.50'))\n",
      "(148, 'Bakery', 'Sandwich', 'FORMULE SANDWICH', Decimal('6.50'))\n",
      "(149, 'Bakery', 'Sandwich', 'DIVERS SANDWICHS', Decimal('2.50'))\n",
      "(150, 'Bakery', 'Viennoiserie', 'CROISSANT AMANDES', Decimal('1.40'))\n",
      "(151, 'Bakery', 'Bread', 'PAIN CHOCO AMANDES', Decimal('1.50'))\n",
      "(152, 'Bakery', 'Viennoiserie', 'SACHET VIENNOISERIE', Decimal('4.00'))\n",
      "(153, 'Bakery', 'Other', 'NANTAIS', Decimal('2.50'))\n",
      "(154, 'Bakery', 'Confectionery', 'CHOCOLAT', Decimal('1.50'))\n",
      "(155, 'Bakery', 'Bread', 'PAIN S/SEL', Decimal('1.50'))\n",
      "(156, 'Bakery', 'Confectionery', 'FONDANT CHOCOLAT', Decimal('2.50'))\n",
      "(157, 'Bakery', 'Other', 'GAL POIRE CHOCO 6P', Decimal('12.00'))\n",
      "(158, 'Bakery', 'Other', 'GAL POIRE CHOCO 4P', Decimal('8.00'))\n",
      "(159, 'Bakery', 'Other', 'GALETTE 8 PERS', Decimal('16.00'))\n",
      "(160, 'Bakery', 'Sandwich', 'SAND JB', Decimal('3.00'))\n",
      "(161, 'Bakery', 'Other', 'SACHET DE CROUTON', Decimal('1.60'))\n",
      "(162, 'Bakery', 'Confectionery', 'GRANDE SUCETTE', Decimal('0.60'))\n",
      "(163, 'Bakery', 'Bread', 'DEMI PAIN', Decimal('0.60'))\n",
      "(164, 'Bakery', 'Pâtisserie', 'TARTELETTE', Decimal('2.00'))\n",
      "(165, 'Bakery', 'Other', 'FLAN', Decimal('2.00'))\n",
      "(166, 'Bakery', 'Pâtisserie', 'PARIS BREST', Decimal('2.00'))\n",
      "(167, 'Bakery', 'Other', 'SAVARIN', Decimal('2.00'))\n",
      "(168, 'Bakery', 'Other', 'FLAN ABRICOT', Decimal('2.10'))\n",
      "(169, 'Bakery', 'Bread', 'BAGUETTE APERO', Decimal('4.50'))\n",
      "(170, 'Bakery', 'Pâtisserie', 'MILLES FEUILLES', Decimal('2.00'))\n",
      "(171, 'Bakery', 'Other', 'CHOU CHANTILLY', Decimal('2.00'))\n",
      "(172, 'Bakery', 'Pâtisserie', 'ECLAIR', Decimal('2.00'))\n",
      "(173, 'Bakery', 'Pâtisserie', 'ROYAL 4P', Decimal('12.00'))\n",
      "(174, 'Bakery', 'Pâtisserie', 'TARTE FRUITS 6P', Decimal('12.00'))\n",
      "(175, 'Bakery', 'Pâtisserie', 'TARTE FRUITS 4P', Decimal('9.00'))\n",
      "(176, 'Bakery', 'Other', 'NOIX JAPONAISE', Decimal('2.00'))\n",
      "(177, 'Tea', 'Tea', 'THE', Decimal('1.50'))\n",
      "(178, 'Bakery', 'Viennoiserie', 'BRIOCHETTE', Decimal('0.80'))\n",
      "(179, 'Bakery', 'Pâtisserie', 'ROYAL 6P', Decimal('18.00'))\n",
      "(180, 'Bakery', 'Pâtisserie', 'ECLAIR FRAISE PISTACHE', Decimal('2.10'))\n",
      "(181, 'Bakery', 'Other', '.', Decimal('0.00'))\n",
      "(182, 'Bakery', 'Other', 'GD FAR BRETON', Decimal('0.00'))\n",
      "(183, 'Bakery', 'Other', 'TRIANGLES', Decimal('2.50'))\n",
      "(184, 'Bakery', 'Pâtisserie', 'TROPEZIENNE', Decimal('2.00'))\n",
      "(185, 'Bakery', 'Pâtisserie', 'TROPEZIENNE FRAMBOISE', Decimal('2.10'))\n",
      "(186, 'Bakery', 'Pâtisserie', 'ROYAL', Decimal('3.00'))\n",
      "(187, 'Bakery', 'Pâtisserie', 'TARTE FRAISE 6P', Decimal('18.00'))\n",
      "(188, 'Bakery', 'Pâtisserie', 'TARTELETTE FRAISE', Decimal('3.00'))\n",
      "(189, 'Bakery', 'Pâtisserie', 'TARTE FRAISE 4PER', Decimal('12.00'))\n",
      "(190, 'Bakery', 'Pâtisserie', 'FRAISIER', Decimal('3.00'))\n",
      "(191, 'Bakery', 'Other', 'NID DE POULE', Decimal('2.50'))\n",
      "(192, 'Bakery', 'Pâtisserie', 'TARTELETTE CHOC', Decimal('2.50'))\n",
      "(193, 'Bakery', 'Bread', 'PAIN DE MIE', Decimal('2.00'))\n",
      "(194, 'Bakery', 'Pâtisserie', 'CRUMBLE', Decimal('2.00'))\n",
      "(195, 'Bakery', 'Pâtisserie', 'FINANCIER', Decimal('0.70'))\n",
      "(196, 'Bakery', 'Beverage', 'DIVERS BOISSONS', Decimal('2.00'))\n",
      "(197, 'Bakery', 'Other', 'CAKE', Decimal('3.00'))\n",
      "(198, 'Bakery', 'Other', 'VIENNOISE', Decimal('1.10'))\n",
      "(199, 'Bakery', 'Meal/Traiteur', 'TRAITEUR', Decimal('1.50'))\n",
      "(200, 'Bakery', 'Bread', 'PAIN GRAINES', Decimal('2.00'))\n",
      "\n",
      "Store Table:\n",
      "(3, 'Astoria')\n",
      "(5, 'Lower Manhattan')\n",
      "(8, \"Hell's Kitchen\")\n",
      "\n",
      "Transactions Table:\n",
      "(1, 150040, datetime.date(2021, 1, 2), datetime.time(8, 38), 1, 3, 101)\n",
      "(2, 150040, datetime.date(2021, 1, 2), datetime.time(8, 38), 3, 3, 102)\n",
      "(3, 150041, datetime.date(2021, 1, 2), datetime.time(9, 14), 2, 5, 102)\n",
      "(4, 150041, datetime.date(2021, 1, 2), datetime.time(9, 14), 1, 5, 103)\n",
      "(5, 150042, datetime.date(2021, 1, 2), datetime.time(9, 25), 5, 3, 104)\n",
      "(6, 150043, datetime.date(2021, 1, 2), datetime.time(9, 25), 2, 3, 101)\n",
      "(7, 150043, datetime.date(2021, 1, 2), datetime.time(9, 25), 3, 3, 105)\n",
      "(8, 150044, datetime.date(2021, 1, 2), datetime.time(9, 27), 1, 5, 106)\n",
      "(9, 150045, datetime.date(2021, 1, 2), datetime.time(9, 32), 3, 5, 104)\n",
      "(10, 150045, datetime.date(2021, 1, 2), datetime.time(9, 32), 6, 5, 105)\n",
      "(11, 150046, datetime.date(2021, 1, 2), datetime.time(9, 37), 6, 3, 102)\n",
      "(12, 150046, datetime.date(2021, 1, 2), datetime.time(9, 37), 6, 3, 105)\n",
      "(13, 150046, datetime.date(2021, 1, 2), datetime.time(9, 37), 6, 3, 104)\n",
      "(14, 150048, datetime.date(2021, 1, 2), datetime.time(9, 39), 3, 8, 105)\n",
      "(15, 150049, datetime.date(2021, 1, 2), datetime.time(9, 40), 2, 3, 105)\n",
      "(16, 150049, datetime.date(2021, 1, 2), datetime.time(9, 40), 1, 3, 104)\n",
      "(17, 150050, datetime.date(2021, 1, 2), datetime.time(9, 41), 2, 3, 104)\n",
      "(18, 150051, datetime.date(2021, 1, 2), datetime.time(9, 46), 1, 3, 103)\n",
      "(19, 150052, datetime.date(2021, 1, 2), datetime.time(9, 48), 1, 3, 107)\n",
      "(20, 150052, datetime.date(2021, 1, 2), datetime.time(9, 48), 1, 3, 108)\n",
      "(21, 150052, datetime.date(2021, 1, 2), datetime.time(9, 48), 1, 3, 109)\n",
      "(22, 150053, datetime.date(2021, 1, 2), datetime.time(9, 48), 1, 5, 104)\n",
      "(23, 150053, datetime.date(2021, 1, 2), datetime.time(9, 48), 2, 5, 110)\n",
      "(24, 150054, datetime.date(2021, 1, 2), datetime.time(9, 49), 1, 3, 111)\n",
      "(25, 150054, datetime.date(2021, 1, 2), datetime.time(9, 49), 2, 3, 103)\n",
      "(26, 150055, datetime.date(2021, 1, 2), datetime.time(9, 57), 4, 8, 104)\n",
      "(27, 150056, datetime.date(2021, 1, 2), datetime.time(9, 58), 3, 5, 106)\n",
      "(28, 150056, datetime.date(2021, 1, 2), datetime.time(9, 58), 5, 5, 105)\n",
      "(29, 150058, datetime.date(2021, 1, 2), datetime.time(10, 3), 2, 8, 104)\n",
      "(30, 150058, datetime.date(2021, 1, 2), datetime.time(10, 3), 2, 8, 105)\n",
      "(31, 150058, datetime.date(2021, 1, 2), datetime.time(10, 3), 2, 8, 102)\n",
      "(32, 150059, datetime.date(2021, 1, 2), datetime.time(10, 4), 4, 8, 104)\n",
      "(33, 150060, datetime.date(2021, 1, 2), datetime.time(10, 5), 4, 8, 104)\n",
      "(34, 150061, datetime.date(2021, 1, 2), datetime.time(10, 12), 1, 8, 108)\n",
      "(35, 150061, datetime.date(2021, 1, 2), datetime.time(10, 12), 1, 8, 109)\n",
      "(36, 150061, datetime.date(2021, 1, 2), datetime.time(10, 12), 1, 8, 112)\n",
      "(37, 150061, datetime.date(2021, 1, 2), datetime.time(10, 12), 1, 8, 109)\n",
      "(38, 150062, datetime.date(2021, 1, 2), datetime.time(10, 17), 1, 5, 108)\n",
      "(39, 150062, datetime.date(2021, 1, 2), datetime.time(10, 17), 3, 5, 109)\n",
      "(40, 150062, datetime.date(2021, 1, 2), datetime.time(10, 17), 2, 5, 113)\n",
      "(41, 150063, datetime.date(2021, 1, 2), datetime.time(10, 18), 1, 5, 109)\n",
      "(42, 150063, datetime.date(2021, 1, 2), datetime.time(10, 18), 1, 5, 114)\n",
      "(43, 150063, datetime.date(2021, 1, 2), datetime.time(10, 18), 1, 5, 115)\n",
      "(44, 150063, datetime.date(2021, 1, 2), datetime.time(10, 18), 1, 5, 109)\n",
      "(45, 150063, datetime.date(2021, 1, 2), datetime.time(10, 18), 1, 5, 116)\n",
      "(46, 150063, datetime.date(2021, 1, 2), datetime.time(10, 18), 2, 5, 104)\n",
      "(47, 150064, datetime.date(2021, 1, 2), datetime.time(10, 22), 1, 8, 110)\n",
      "(48, 150065, datetime.date(2021, 1, 2), datetime.time(10, 22), 1, 8, 110)\n",
      "(49, 150065, datetime.date(2021, 1, 2), datetime.time(10, 22), 2, 8, 117)\n",
      "(50, 150066, datetime.date(2021, 1, 2), datetime.time(10, 24), 1, 5, 104)\n",
      "(51, 150066, datetime.date(2021, 1, 2), datetime.time(10, 24), 1, 5, 101)\n",
      "(52, 150066, datetime.date(2021, 1, 2), datetime.time(10, 24), 2, 5, 102)\n",
      "(53, 150066, datetime.date(2021, 1, 2), datetime.time(10, 24), 2, 5, 105)\n",
      "(54, 150067, datetime.date(2021, 1, 2), datetime.time(10, 24), 2, 5, 105)\n",
      "(55, 150067, datetime.date(2021, 1, 2), datetime.time(10, 24), 1, 5, 104)\n",
      "(56, 150068, datetime.date(2021, 1, 2), datetime.time(10, 25), 2, 5, 107)\n",
      "(57, 150069, datetime.date(2021, 1, 2), datetime.time(10, 29), 1, 3, 106)\n",
      "(58, 150069, datetime.date(2021, 1, 2), datetime.time(10, 29), 2, 3, 111)\n",
      "(59, 150069, datetime.date(2021, 1, 2), datetime.time(10, 29), 1, 3, 118)\n",
      "(60, 150069, datetime.date(2021, 1, 2), datetime.time(10, 29), 2, 3, 119)\n",
      "(61, 150070, datetime.date(2021, 1, 2), datetime.time(10, 33), 2, 3, 105)\n",
      "(62, 150070, datetime.date(2021, 1, 2), datetime.time(10, 33), 1, 3, 106)\n",
      "(63, 150070, datetime.date(2021, 1, 2), datetime.time(10, 33), 2, 3, 102)\n",
      "(64, 150071, datetime.date(2021, 1, 2), datetime.time(10, 34), 2, 3, 104)\n",
      "(65, 150071, datetime.date(2021, 1, 2), datetime.time(10, 34), 4, 3, 102)\n",
      "(66, 150071, datetime.date(2021, 1, 2), datetime.time(10, 34), 2, 3, 105)\n",
      "(67, 150072, datetime.date(2021, 1, 2), datetime.time(10, 39), 1, 8, 104)\n",
      "(68, 150073, datetime.date(2021, 1, 2), datetime.time(10, 40), 1, 3, 104)\n",
      "(69, 150074, datetime.date(2021, 1, 2), datetime.time(10, 42), 1, 8, 107)\n",
      "(70, 150075, datetime.date(2021, 1, 2), datetime.time(10, 44), 1, 8, 109)\n",
      "(71, 150075, datetime.date(2021, 1, 2), datetime.time(10, 44), 1, 8, 120)\n",
      "(72, 150076, datetime.date(2021, 1, 2), datetime.time(10, 46), 2, 3, 104)\n",
      "(73, 150077, datetime.date(2021, 1, 2), datetime.time(10, 49), 1, 8, 121)\n",
      "(74, 150077, datetime.date(2021, 1, 2), datetime.time(10, 49), 2, 8, 102)\n",
      "(75, 150077, datetime.date(2021, 1, 2), datetime.time(10, 49), 2, 8, 105)\n",
      "(76, 150078, datetime.date(2021, 1, 2), datetime.time(10, 50), 1, 3, 104)\n",
      "(77, 150078, datetime.date(2021, 1, 2), datetime.time(10, 50), 1, 3, 114)\n",
      "(78, 150079, datetime.date(2021, 1, 2), datetime.time(10, 51), 1, 3, 101)\n",
      "(79, 150079, datetime.date(2021, 1, 2), datetime.time(10, 51), 1, 3, 122)\n",
      "(80, 150079, datetime.date(2021, 1, 2), datetime.time(10, 51), 1, 3, 108)\n",
      "(81, 150079, datetime.date(2021, 1, 2), datetime.time(10, 51), 1, 3, 123)\n",
      "(82, 150080, datetime.date(2021, 1, 2), datetime.time(10, 53), 2, 5, 104)\n",
      "(83, 150080, datetime.date(2021, 1, 2), datetime.time(10, 53), 3, 5, 105)\n",
      "(84, 150081, datetime.date(2021, 1, 2), datetime.time(10, 54), 1, 3, 104)\n",
      "(85, 150082, datetime.date(2021, 1, 2), datetime.time(10, 54), 4, 5, 104)\n",
      "(86, 150083, datetime.date(2021, 1, 2), datetime.time(10, 54), 1, 3, 104)\n",
      "(87, 150084, datetime.date(2021, 1, 2), datetime.time(10, 55), 1, 3, 119)\n",
      "(88, 150084, datetime.date(2021, 1, 2), datetime.time(10, 55), 1, 3, 124)\n",
      "(89, 150084, datetime.date(2021, 1, 2), datetime.time(10, 55), 4, 3, 102)\n",
      "(90, 150084, datetime.date(2021, 1, 2), datetime.time(10, 55), 4, 3, 105)\n",
      "(91, 150085, datetime.date(2021, 1, 2), datetime.time(10, 56), 4, 5, 104)\n",
      "(92, 150086, datetime.date(2021, 1, 2), datetime.time(10, 57), 1, 5, 102)\n",
      "(93, 150086, datetime.date(2021, 1, 2), datetime.time(10, 57), 1, 5, 119)\n",
      "(94, 150086, datetime.date(2021, 1, 2), datetime.time(10, 57), 2, 5, 105)\n",
      "(95, 150087, datetime.date(2021, 1, 2), datetime.time(10, 58), 2, 3, 101)\n",
      "(96, 150088, datetime.date(2021, 1, 2), datetime.time(11, 4), 2, 8, 104)\n",
      "(97, 150089, datetime.date(2021, 1, 2), datetime.time(11, 6), 1, 5, 101)\n",
      "(98, 150090, datetime.date(2021, 1, 2), datetime.time(11, 8), 1, 8, 117)\n",
      "(99, 150090, datetime.date(2021, 1, 2), datetime.time(11, 8), 1, 8, 102)\n",
      "(100, 150090, datetime.date(2021, 1, 2), datetime.time(11, 8), 1, 8, 105)\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "connection = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"OntoDb\",\n",
    "    user=\"ontodb\",\n",
    "    password=\"admin\"\n",
    ")\n",
    "cur = connection.cursor()\n",
    "\n",
    "cur.execute(\"SELECT * FROM Product LIMIT 100;\")\n",
    "product_data = cur.fetchall()\n",
    "print(\"Product Table:\")\n",
    "for row in product_data:\n",
    "    print(row)\n",
    "\n",
    "cur.execute(\"SELECT * FROM Store LIMIT 100;\")\n",
    "store_data = cur.fetchall()\n",
    "print(\"\\nStore Table:\")\n",
    "for row in store_data:\n",
    "    print(row)\n",
    "\n",
    "cur.execute(\"SELECT * FROM Transactions LIMIT 100;\")\n",
    "transaction_data = cur.fetchall()\n",
    "print(\"\\nTransactions Table:\")\n",
    "for row in transaction_data:\n",
    "    print(row)\n",
    "\n",
    "cur.close()\n",
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To clean the tables elements from the db (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"OntoDb\",\n",
    "    user=\"ontodb\",\n",
    "    password=\"admin\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "    TRUNCATE TABLE Product, Store, Transactions \n",
    "    RESTART IDENTITY CASCADE;\n",
    "\"\"\")\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Cleaned all data from the tables.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
