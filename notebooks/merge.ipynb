{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bakery_path = \"../data/raw/Bakery_sales.csv\"\n",
    "coffee_shop_path = \"../data/raw/Coffee_Shop_Sales.csv\"\n",
    "bakery_sales = pd.read_csv(bakery_path)\n",
    "Coffee_sales = pd.read_csv(coffee_shop_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/cleaned\u001b[39m\u001b[38;5;124m\"\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Create directory if it doesn't exist\u001b[39;00m\n\u001b[0;32m      2\u001b[0m cleaned_csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/cleaned/Coffee_Shop_Sales_cleaned.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m Coffee_sales\u001b[38;5;241m.\u001b[39mto_csv(cleaned_csv_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs(\"../data/cleaned\", exist_ok=True)  # Create directory if it doesn't exist\n",
    "cleaned_csv_path = \"../data/cleaned/Coffee_Shop_Sales_cleaned.csv\"\n",
    "Coffee_sales.to_csv(cleaned_csv_path, index=False)\n",
    "print(f\"Saved cleaned file to: {cleaned_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the unit_price column by removing the € sign and converting the column to float\n",
    "bakery_sales[\"unit_price\"] = (\n",
    "    bakery_sales[\"unit_price\"]\n",
    "    .str.replace(\"€\", \"\", regex=False)\n",
    "    .str.replace(\",\", \".\", regex=False)\n",
    "    .str.replace(\" \", \"\", regex=False)\n",
    "    .str.replace('\"', \"\", regex=False)\n",
    "    .str.strip()\n",
    "    .astype(float)\n",
    ")\n",
    "#rename the columns to match the target schema\n",
    "bakery_sales = bakery_sales.rename(\n",
    "    columns={\n",
    "        \"ticket_number\": \"transaction_id\",\n",
    "        \"date\": \"transaction_date\",\n",
    "        \"time\": \"transaction_time\",\n",
    "        \"Quantity\": \"transaction_qty\",\n",
    "        \"article\": \"product_detail\",\n",
    "    }\n",
    ")\n",
    "#add the missing columns\n",
    "bakery_sales[\"store_id\"] = None\n",
    "bakery_sales[\"store_location\"] = None\n",
    "bakery_sales[\"product_id\"] = None\n",
    "bakery_sales[\"product_category\"] = \"Bakery\"\n",
    "bakery_sales[\"product_type\"] = None\n",
    "#reorder the columns\n",
    "bakery_sales = bakery_sales[\n",
    "    [\n",
    "        \"transaction_id\",\n",
    "        \"transaction_date\",\n",
    "        \"transaction_time\",\n",
    "        \"transaction_qty\",\n",
    "        \"store_id\",\n",
    "        \"store_location\",\n",
    "        \"product_id\",\n",
    "        \"unit_price\",\n",
    "        \"product_category\",\n",
    "        \"product_type\",\n",
    "        \"product_detail\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Save the cleaned bakery sales data to the cleaned folder\n",
    "bakery_cleaned_csv_path = \"../data/cleaned/Bakery_sales_cleaned.csv\"\n",
    "bakery_sales.to_csv(bakery_cleaned_csv_path, index=False)\n",
    "print(f\"Saved cleaned bakery sales file to: {bakery_cleaned_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_categories = Coffee_sales['product_category'].unique()\n",
    "print(unique_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bakery_items = Coffee_sales[Coffee_sales['product_category'] == 'Bakery'][['product_type', 'product_detail']].drop_duplicates()\n",
    "print(bakery_items.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bakery_sales_new = pd.read_csv(bakery_cleaned_csv_path)\n",
    "bakery_items_new = bakery_sales_new[bakery_sales_new['product_category'] == 'Bakery'][['product_detail']].drop_duplicates()\n",
    "print(bakery_items_new['product_detail'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def map_bakery_product_types(df):\n",
    "    \"\"\"Map bakery product details to appropriate product types\"\"\"\n",
    "    \n",
    "    # Define category mappings - expanded with more comprehensive terms\n",
    "    bread_types = ['BAGUETTE', 'PAIN', 'BANETTE', 'BOULE', 'COMPLET', 'CAMPAGNE', \n",
    "                  'SEIGLE', 'MOISSON', 'CEREAL', 'POLKA', 'FICELLE', 'TRADITION', \n",
    "                  'SPECIAL BREAD', 'VIK BREAD', 'QUIM BREAD', 'PAIN BANETTE', 'NOIR',\n",
    "                  'BOULANGERIE', 'MIE', 'CEREAL BAGUETTE', 'GRAINE', 'PAIN S/SEL']\n",
    "    \n",
    "    viennoiserie_types = ['PAIN AU CHOCOLAT', 'CROISSANT', 'KOUIGN AMANN', 'PAIN AUX RAISINS', \n",
    "                         'BRIOCHE', 'CHAUSSON', 'VIENNOISERIE', 'BRIOCHETTE', 'PAIN SUISSE',\n",
    "                         'PALMIER', 'PAILLE', 'TULIPE']\n",
    "    \n",
    "    pastry_types = ['GAL', 'GALETTE', 'TARTE', 'FRANGIPANE', 'POMME', 'FAR BRETON', \n",
    "                   'TARTELETTE', 'KOUIGN', 'PATISSERIE', 'FLAN', 'PARIS BREST', 'SAVARIN',\n",
    "                   'MILLES FEUILLES', 'CHOU', 'ECLAIR', 'ROYAL', 'TROPEZIENNE', 'FRAISIER',\n",
    "                   'NID DE POULE', 'CRUMBLE', 'FINANCIER', 'CAKE', 'ST HONORE', 'BROWNIES',\n",
    "                   'RELIGIEUSE', 'DELICE', 'ENTREMETS', 'BUCHE']\n",
    "    \n",
    "    sandwich_types = ['SAND', 'SANDWICH', 'FORMULE SANDWICH']\n",
    "    \n",
    "    cookie_types = ['COOKIE', 'SABLE', 'PALET BRETON', 'NANTAIS', 'MERINGUE']\n",
    "    \n",
    "    beverage_types = ['CAFE', 'EAU', 'THE', 'BOISSON', 'CHOCOLAT']\n",
    "    \n",
    "    confectionery_types = ['CONFISERIE', 'SUCETTE', 'MACARON', 'CARAMEL']\n",
    "    \n",
    "    # Create a function to determine product type\n",
    "    def get_product_type(product_detail):\n",
    "        if pd.isna(product_detail):\n",
    "            return \"Unknown\"\n",
    "        \n",
    "        product = str(product_detail).upper()\n",
    "        \n",
    "        if any(bread in product for bread in bread_types):\n",
    "            return \"Bread\"\n",
    "        elif any(item in product for item in viennoiserie_types):\n",
    "            return \"Viennoiserie\"\n",
    "        elif any(pastry in product for pastry in pastry_types):\n",
    "            return \"Pastry\"\n",
    "        elif any(sandwich in product for sandwich in sandwich_types):\n",
    "            return \"Sandwich\"\n",
    "        elif any(cookie in product for cookie in cookie_types):\n",
    "            return \"Cookie\"\n",
    "        elif any(beverage in product for beverage in beverage_types):\n",
    "            return \"Beverage\"\n",
    "        elif any(confectionery in product for confectionery in confectionery_types):\n",
    "            return \"Confectionery\"\n",
    "        elif \"COUPE\" in product:\n",
    "            return \"Service\"\n",
    "        elif \"PLATPREPARE\" in product or \"PLAT\" in product or \"TRAITEUR\" in product:\n",
    "            return \"Prepared Food\"\n",
    "        elif \"REDUCTION\" in product or \"PLATEAU\" in product:\n",
    "            return \"Assortment\"\n",
    "        else:\n",
    "            return \"Other\"\n",
    "    \n",
    "    # Apply the function to create product_type column\n",
    "    df['product_type'] = df['product_detail'].apply(get_product_type)\n",
    "    \n",
    "    return df\n",
    "\n",
    "cleaned_bakery_path = bakery_cleaned_csv_path  \n",
    "\n",
    "print(f\"Loading bakery data from: {cleaned_bakery_path}\")\n",
    "bakery_sales_cleaned = pd.read_csv(cleaned_bakery_path)\n",
    "\n",
    "bakery_sales_cleaned = map_bakery_product_types(bakery_sales_cleaned)\n",
    "\n",
    "print(bakery_sales_cleaned[['product_detail', 'product_type']].head(10))\n",
    "\n",
    "print(\"\\nProduct Type Distribution:\")\n",
    "print(bakery_sales_cleaned['product_type'].value_counts())\n",
    "\n",
    "bakery_sales_cleaned.to_csv(cleaned_bakery_path, index=False)\n",
    "print(f\"Updated bakery data saved to: {cleaned_bakery_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Create a mapping of product names to product IDs.\n",
    "raw_product_names = [\n",
    "    'BAGUETTE', 'PAIN AU CHOCOLAT', 'PAIN', 'TRADITIONAL BAGUETTE', 'CROISSANT',\n",
    "    'BANETTE', 'BANETTINE', 'SPECIAL BREAD', 'COUPE', 'SAND JB EMMENTAL',\n",
    "    'KOUIGN AMANN', 'BOULE 200G', 'BOULE 400G', 'GAL FRANGIPANE 6P', 'CAMPAGNE',\n",
    "    'MOISSON', 'CAFE OU EAU', 'BRIOCHE', 'CEREAL BAGUETTE', 'SEIGLE', 'COMPLET',\n",
    "    'DIVERS PATISSERIE', 'GAL FRANGIPANE 4P', 'COOKIE', 'FICELLE',\n",
    "    'PAIN AUX RAISINS', 'GAL POMME 6P', 'GAL POMME 4P', 'FINANCIER X5',\n",
    "    'VIK BREAD', 'DIVERS VIENNOISERIE', 'GACHE', 'SANDWICH COMPLET',\n",
    "    'PAIN BANETTE', 'GRAND FAR BRETON', 'QUIM BREAD', 'SPECIAL BREAD KG',\n",
    "    'GD KOUIGN AMANN', 'BOULE POLKA', 'DEMI BAGUETTE', 'CHAUSSON AUX POMMES',\n",
    "    'BAGUETTE GRAINE', 'DIVERS CONFISERIE', 'SUCETTE', 'DIVERS BOULANGERIE',\n",
    "    'BOISSON 33CL', 'PATES', 'FORMULE SANDWICH', 'DIVERS SANDWICHS',\n",
    "    'CROISSANT AMANDES', 'PAIN CHOCO AMANDES', 'SACHET VIENNOISERIE', 'NANTAIS',\n",
    "    'CHOCOLAT', 'PAIN S/SEL', 'FONDANT CHOCOLAT', 'GAL POIRE CHOCO 6P',\n",
    "    'GAL POIRE CHOCO 4P', 'GALETTE 8 PERS', 'SAND JB', 'SACHET DE CROUTON',\n",
    "    'GRANDE SUCETTE', 'DEMI PAIN', 'TARTELETTE', 'FLAN', 'PARIS BREST', 'SAVARIN',\n",
    "    'FLAN ABRICOT', 'BAGUETTE APERO', 'MILLES FEUILLES', 'CHOU CHANTILLY',\n",
    "    'ECLAIR', 'ROYAL 4P', 'TARTE FRUITS 6P', 'TARTE FRUITS 4P', 'NOIX JAPONAISE',\n",
    "    'THE', 'BRIOCHETTE', 'ROYAL 6P', 'ECLAIR FRAISE PISTACHE', '.',\n",
    "    'GD FAR BRETON', 'TRIANGLES', 'TROPEZIENNE', 'TROPEZIENNE FRAMBOISE', 'ROYAL',\n",
    "    'TARTE FRAISE 6P', 'TARTELETTE FRAISE', 'TARTE FRAISE 4PER', 'FRAISIER',\n",
    "    'NID DE POULE', 'TARTELETTE CHOC', 'PAIN DE MIE', 'CRUMBLE', 'FINANCIER',\n",
    "    'DIVERS BOISSONS', 'CAKE', 'VIENNOISE', 'TRAITEUR', 'PAIN GRAINES',\n",
    "    'PLATPREPARE6,50', 'PLATPREPARE5,50', 'PLATPREPARE7,00',\n",
    "    'FORMULE PLAT PREPARE', 'ST HONORE', 'BROWNIES', 'RELIGIEUSE',\n",
    "    'PLATPREPARE6,00', 'DELICETROPICAL', 'CRUMBLECARAMEL OU PISTAE',\n",
    "    'PT NANTAIS', 'GD NANTAIS', 'DOUCEUR D HIVER', 'TROIS CHOCOLAT',\n",
    "    'ARTICLE 295', 'TARTE FINE', 'ENTREMETS', 'BRIOCHE DE NOEL', 'FRAMBOISIER',\n",
    "    'BUCHE 4PERS', 'BUCHE 6PERS', 'GD PLATEAU SALE', 'BUCHE 8PERS',\n",
    "    'PT PLATEAU SALE', 'REDUCTION SUCREES 12', 'PAIN NOIR',\n",
    "    'REDUCTION SUCREES 24', 'BOTTEREAU', 'MERINGUE', 'PALMIER', 'PAILLE',\n",
    "    'PLAT 6.50E', 'PLAT 7.60E', 'PLAT 7.00', 'PLAT', 'PLAT 8.30E', 'FORMULE PATE',\n",
    "    'GUERANDAIS', 'PALET BRETON', 'CARAMEL NOIX', 'MACARON', '12 MACARON',\n",
    "    'ARMORICAIN', 'PLAQUE TARTE 25P', 'SABLE F  P', 'PAIN SUISSE PEPITO',\n",
    "    'TULIPE', 'TARTELETTE COCKTAIL', 'SACHET DE VIENNOISERIE'\n",
    "]\n",
    "\n",
    "# Clean names: upper case and remove spaces.\n",
    "clean_product_names = [name.strip().upper() for name in raw_product_names]\n",
    "\n",
    "# Create a dictionary where each product name is mapped to a unique id.\n",
    "product_map = {name: idx for idx, name in enumerate(clean_product_names, start=101)}\n",
    "\n",
    "# Show the mapping information.\n",
    "print(\"Product Mapping:\")\n",
    "for product, pid in product_map.items():\n",
    "    print(f\"{product}: {pid}\")\n",
    "\n",
    "# Read the cleaned sales file using the path variable that already exists\n",
    "df = pd.read_csv(bakery_cleaned_csv_path)\n",
    "\n",
    "# Check missing values before fixing data.\n",
    "print(\"\\nMissing values before cleaning:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Clean product details and format the string.\n",
    "df['product_detail'] = df['product_detail'].astype(str).str.strip().str.upper()\n",
    "\n",
    "# Ensure product_id exists and fill missing ones with 0.\n",
    "if 'product_id' not in df.columns:\n",
    "    df['product_id'] = 0\n",
    "else:\n",
    "    df['product_id'] = df['product_id'].fillna(0)\n",
    "\n",
    "# Map the product details to product IDs using the dictionary.\n",
    "df['product_id'] = df.apply(\n",
    "    lambda row: product_map.get(row['product_detail'], 999) if row['product_id'] == 0 else row['product_id'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Convert transaction ids to integer type.\n",
    "df['transaction_id'] = df['transaction_id'].astype(int)\n",
    "\n",
    "# Show missing values after cleaning.\n",
    "print(\"\\nMissing values after cleaning:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_stores = Coffee_sales[['store_id', 'store_location']].drop_duplicates()\n",
    "unique_tickets = bakery_sales['transaction_id'].unique()\n",
    "\n",
    "# Attribute a random store to each transaction\n",
    "np.random.seed(42)\n",
    "ticket_store_mapping = {ticket: np.random.choice(unique_stores['store_id']) for ticket in unique_tickets}\n",
    "\n",
    "df['store_id'] = bakery_sales['transaction_id'].map(ticket_store_mapping)\n",
    "df['store_location'] = df['store_id'].map(unique_stores.set_index('store_id')['store_location'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = bakery_cleaned_csv_path\n",
    "df.to_csv(output_filename, index=False)\n",
    "print(f\"\\nCleaning complete. Cleaned data saved as '{output_filename}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Full path variables using raw strings\n",
    "bakery_cleaned_csv_path = r\"C:\\Users\\queri\\Documents\\INFO9014-1\\data\\cleaned\\Bakery_sales_cleaned.csv\"\n",
    "coffee_shop_cleaned_csv_path = r\"C:\\Users\\queri\\Documents\\INFO9014-1\\data\\cleaned\\Coffee_Shop_Sales_cleaned.csv\"\n",
    "\n",
    "# Read the data from the CSV files using the defined variables\n",
    "df_bakery = pd.read_csv(bakery_cleaned_csv_path)\n",
    "df_coffee = pd.read_csv(coffee_shop_cleaned_csv_path)\n",
    "\n",
    "# Assign sales IDs:\n",
    "# Coffee shop sales IDs start from 1\n",
    "df_coffee['sales_id'] = range(1, len(df_coffee) + 1)\n",
    "# Bakery sales IDs start immediately after the last coffee shop ID\n",
    "df_bakery['sales_id'] = range(len(df_coffee) + 1, len(df_coffee) + len(df_bakery) + 1)\n",
    "\n",
    "# Write the updated DataFrames back to the original CSV paths\n",
    "df_bakery.to_csv(bakery_cleaned_csv_path, index=False)\n",
    "df_coffee.to_csv(coffee_shop_cleaned_csv_path, index=False)\n",
    "\n",
    "# Print the range of sales IDs for each DataFrame\n",
    "print(\"Coffee sales IDs range:\", df_coffee['sales_id'].iloc[0], \"to\", df_coffee['sales_id'].iloc[-1])\n",
    "print(\"Bakery sales IDs range:\", df_bakery['sales_id'].iloc[0], \"to\", df_bakery['sales_id'].iloc[-1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
